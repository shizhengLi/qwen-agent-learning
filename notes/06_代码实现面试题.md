# Qwen-Agent 代码实现面试题

## 1. 核心类实现

### 1.1 Agent基类实现

**问题1：请实现一个简单的Agent基类，包含基本的流式处理和消息处理功能。**

**参考答案：**
```python
from abc import ABC, abstractmethod
from typing import List, Iterator, Union, Dict, Optional
import copy
from dataclasses import dataclass

@dataclass
class Message:
    role: str
    content: str
    name: Optional[str] = None

class Agent(ABC):
    def __init__(self, 
                 name: str = "agent",
                 description: str = "A general agent",
                 system_message: Optional[str] = None):
        self.name = name
        self.description = description
        self.system_message = system_message
    
    def run(self, messages: List[Union[Dict, Message]], **kwargs) -> Iterator[List[Message]]:
        """
        模板方法模式：定义统一的处理流程
        """
        # 1. 消息预处理
        messages = copy.deepcopy(messages)
        messages = self._preprocess_messages(messages)
        
        # 2. 系统消息处理
        if self.system_message:
            messages = self._prepend_system_message(messages)
        
        # 3. 调用子类实现的核心逻辑
        yield from self._run(messages=messages, **kwargs)
    
    def _preprocess_messages(self, messages: List[Union[Dict, Message]]) -> List[Message]:
        """
        消息预处理：统一消息格式
        """
        processed_messages = []
        for msg in messages:
            if isinstance(msg, dict):
                processed_messages.append(Message(**msg))
            else:
                processed_messages.append(msg)
        return processed_messages
    
    def _prepend_system_message(self, messages: List[Message]) -> List[Message]:
        """
        添加系统消息
        """
        system_msg = Message(role="system", content=self.system_message)
        return [system_msg] + messages
    
    @abstractmethod
    def _run(self, messages: List[Message], **kwargs) -> Iterator[List[Message]]:
        """
        抽象方法：子类需要实现具体的Agent逻辑
        """
        raise NotImplementedError
    
    def __repr__(self):
        return f"{self.__class__.__name__}(name='{self.name}', description='{self.description}')"
```

**关键点：**
1. 使用抽象基类定义统一接口
2. 模板方法模式实现标准化流程
3. 支持流式处理
4. 消息预处理和格式化
5. 类型安全和错误处理

---

**问题2：请实现一个支持工具调用的Agent，包含工具解析和执行功能。**

**参考答案：**
```python
import json
import inspect
from typing import List, Iterator, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import random

class Tool:
    def __init__(self, name: str, description: str, parameters: Dict[str, Any]):
        self.name = name
        self.description = description
        self.parameters = parameters
    
    def call(self, params: Dict[str, Any]) -> str:
        """
        工具调用方法，子类需要实现
        """
        raise NotImplementedError

class FunctionCallAgent(Agent):
    def __init__(self, 
                 name: str = "function_call_agent",
                 description: str = "Agent with function calling capability",
                 system_message: Optional[str] = None,
                 tools: Optional[List[Tool]] = None,
                 max_workers: int = 5):
        super().__init__(name, description, system_message)
        self.tools = tools or []
        self.max_workers = max_workers
        self.tool_registry = {tool.name: tool for tool in self.tools}
    
    def _run(self, messages: List[Message], **kwargs) -> Iterator[List[Message]]:
        """
        实现带工具调用的Agent逻辑
        """
        # 1. 调用LLM获取响应
        llm_response = self._call_llm(messages)
        
        # 2. 解析工具调用
        function_calls = self._parse_function_calls(llm_response)
        
        if not function_calls:
            # 没有工具调用，直接返回LLM响应
            yield [Message(role="assistant", content=llm_response)]
            return
        
        # 3. 并行执行工具调用
        tool_results = yield from self._execute_tools_parallel(function_calls)
        
        # 4. 将工具结果整合到对话历史
        updated_messages = self._integrate_tool_results(messages, llm_response, tool_results)
        
        # 5. 再次调用LLM生成最终响应
        final_response = self._call_llm(updated_messages)
        yield [Message(role="assistant", content=final_response)]
    
    def _call_llm(self, messages: List[Message]) -> str:
        """
        模拟LLM调用
        """
        # 这里应该是实际的LLM调用逻辑
        # 为了示例，我们返回一个包含工具调用的响应
        if len(self.tools) > 0:
            return f"我需要调用工具来回答这个问题。让我调用 {self.tools[0].name} 工具。"
        return "这是一个普通的LLM响应。"
    
    def _parse_function_calls(self, response: str) -> List[Dict[str, Any]]:
        """
        解析LLM响应中的工具调用
        """
        # 这里应该使用实际的解析逻辑
        # 为了示例，我们返回一个模拟的工具调用
        if self.tools and "调用工具" in response:
            return [{
                "name": self.tools[0].name,
                "arguments": json.dumps({"query": "example query"})
            }]
        return []
    
    def _execute_tools_parallel(self, function_calls: List[Dict[str, Any]]) -> Iterator[List[Message]]:
        """
        并行执行工具调用
        """
        results = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # 提交所有任务
            future_to_call = {}
            for call in function_calls:
                tool_name = call["name"]
                if tool_name in self.tool_registry:
                    tool = self.tool_registry[tool_name]
                    try:
                        args = json.loads(call["arguments"])
                        future = executor.submit(tool.call, args)
                        future_to_call[future] = call
                    except json.JSONDecodeError:
                        results.append({
                            "tool_name": tool_name,
                            "error": "Invalid JSON arguments"
                        })
            
            # 收集结果
            for future in as_completed(future_to_call):
                call = future_to_call[future]
                try:
                    result = future.result()
                    results.append({
                        "tool_name": call["name"],
                        "result": result
                    })
                except Exception as e:
                    results.append({
                        "tool_name": call["name"],
                        "error": str(e)
                    })
                
                # 流式返回中间结果
                yield [Message(role="function", content=json.dumps(results[-1]))]
    
    def _integrate_tool_results(self, 
                               original_messages: List[Message], 
                               llm_response: str, 
                               tool_results: List[Dict[str, Any]]) -> List[Message]:
        """
        将工具结果整合到对话历史
        """
        updated_messages = original_messages.copy()
        
        # 添加LLM的初始响应
        updated_messages.append(Message(role="assistant", content=llm_response))
        
        # 添加工具调用结果
        for result in tool_results:
            role = "function"
            content = json.dumps(result)
            updated_messages.append(Message(role=role, content=content))
        
        return updated_messages
    
    def add_tool(self, tool: Tool):
        """
        添加工具
        """
        self.tools.append(tool)
        self.tool_registry[tool.name] = tool
    
    def remove_tool(self, tool_name: str):
        """
        移除工具
        """
        if tool_name in self.tool_registry:
            tool = self.tool_registry[tool_name]
            self.tools.remove(tool)
            del self.tool_registry[tool_name]

# 示例工具实现
class WeatherTool(Tool):
    def __init__(self):
        super().__init__(
            name="get_weather",
            description="Get weather information for a city",
            parameters={
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "City name"
                    }
                },
                "required": ["city"]
            }
        )
    
    def call(self, params: Dict[str, Any]) -> str:
        city = params.get("city", "Unknown")
        # 模拟API调用
        time.sleep(0.5)  # 模拟网络延迟
        return f"The weather in {city} is sunny, 25°C"

# 使用示例
if __name__ == "__main__":
    # 创建工具
    weather_tool = WeatherTool()
    
    # 创建Agent
    agent = FunctionCallAgent(
        name="weather_agent",
        description="Agent that can answer weather questions",
        system_message="You are a helpful assistant that can provide weather information.",
        tools=[weather_tool]
    )
    
    # 测试Agent
    messages = [
        Message(role="user", content="What's the weather in Beijing?")
    ]
    
    print("Running agent...")
    for response_chunk in agent.run(messages):
        for msg in response_chunk:
            print(f"{msg.role}: {msg.content}")
```

**关键点：**
1. 并行工具调用提高效率
2. 错误处理和异常管理
3. 消息历史的维护和整合
4. 流式输出支持
5. 工具注册和管理

---

### 1.2 LLM接口实现

**问题3：请实现一个支持多种LLM提供商的统一接口，包含流式处理和缓存功能。**

**参考答案：**
```python
import asyncio
import json
import time
import hashlib
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, Union, Iterator
from dataclasses import dataclass
import aiohttp
import redis

@dataclass
class LLMConfig:
    model_type: str
    model: str
    api_key: str
    base_url: Optional[str] = None
    generate_cfg: Optional[Dict[str, Any]] = None

class BaseLLM(ABC):
    def __init__(self, config: LLMConfig):
        self.config = config
        self.cache = None
        self.session = None
    
    def set_cache(self, cache_client):
        """
        设置缓存客户端
        """
        self.cache = cache_client
    
    def chat(self, 
             messages: List[Dict[str, Any]], 
             stream: bool = False,
             **kwargs) -> Union[str, Iterator[str]]:
        """
        统一的聊天接口
        """
        if stream:
            return self._chat_stream(messages, **kwargs)
        else:
            return self._chat_no_stream(messages, **kwargs)
    
    @abstractmethod
    def _chat_no_stream(self, messages: List[Dict[str, Any]], **kwargs) -> str:
        """
        非流式聊天接口
        """
        raise NotImplementedError
    
    @abstractmethod
    def _chat_stream(self, messages: List[Dict[str, Any]], **kwargs) -> Iterator[str]:
        """
        流式聊天接口
        """
        raise NotImplementedError
    
    def _generate_cache_key(self, messages: List[Dict[str, Any]], **kwargs) -> str:
        """
        生成缓存键
        """
        cache_data = {
            "messages": messages,
            "config": self.config.generate_cfg,
            "kwargs": kwargs
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[str]:
        """
        从缓存获取结果
        """
        if self.cache is None:
            return None
        
        try:
            return self.cache.get(cache_key)
        except Exception as e:
            print(f"Cache get error: {e}")
            return None
    
    def _set_to_cache(self, cache_key: str, value: str, expire: int = 3600):
        """
        设置缓存
        """
        if self.cache is None:
            return
        
        try:
            self.cache.setex(cache_key, expire, value)
        except Exception as e:
            print(f"Cache set error: {e}")

class QwenLLM(BaseLLM):
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        self.headers = {
            "Authorization": f"Bearer {config.api_key}",
            "Content-Type": "application/json"
        }
    
    async def _get_session(self):
        """
        获取HTTP会话
        """
        if self.session is None or self.session.closed:
            connector = aiohttp.TCPConnector(limit=100, limit_per_host=30)
            self.session = aiohttp.ClientSession(connector=connector)
        return self.session
    
    def _chat_no_stream(self, messages: List[Dict[str, Any]], **kwargs) -> str:
        """
        Qwen非流式聊天实现
        """
        cache_key = self._generate_cache_key(messages, **kwargs)
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            return cached_result
        
        # 准备请求数据
        request_data = {
            "model": self.config.model,
            "input": {
                "messages": messages
            },
            "parameters": self.config.generate_cfg or {}
        }
        
        # 同步调用（实际项目中应该使用异步）
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(self._make_request(request_data))
            self._set_to_cache(cache_key, result)
            return result
        finally:
            loop.close()
    
    def _chat_stream(self, messages: List[Dict[str, Any]], **kwargs) -> Iterator[str]:
        """
        Qwen流式聊天实现
        """
        request_data = {
            "model": self.config.model,
            "input": {
                "messages": messages
            },
            "parameters": {
                **(self.config.generate_cfg or {}),
                "incremental_output": True
            }
        }
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            for chunk in loop.run_until_complete(self._make_stream_request(request_data)):
                yield chunk
        finally:
            loop.close()
    
    async def _make_request(self, data: Dict[str, Any]) -> str:
        """
        发送HTTP请求
        """
        session = await self._get_session()
        
        try:
            async with session.post(self.base_url, headers=self.headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["output"]["text"]
                else:
                    raise Exception(f"Request failed: {response.status}")
        except Exception as e:
            print(f"Request error: {e}")
            raise
    
    async def _make_stream_request(self, data: Dict[str, Any]) -> Iterator[str]:
        """
        发送流式HTTP请求
        """
        session = await self._get_session()
        
        try:
            async with session.post(self.base_url, headers=self.headers, json=data) as response:
                if response.status == 200:
                    async for line in response.content:
                        line = line.decode('utf-8').strip()
                        if line.startswith('data: '):
                            data_str = line[6:]
                            if data_str != '[DONE]':
                                try:
                                    data = json.loads(data_str)
                                    if 'output' in data and 'text' in data['output']:
                                        yield data['output']['text']
                                except json.JSONDecodeError:
                                    continue
                else:
                    raise Exception(f"Stream request failed: {response.status}")
        except Exception as e:
            print(f"Stream request error: {e}")
            raise

class OpenAILLM(BaseLLM):
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.openai.com/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {config.api_key}",
            "Content-Type": "application/json"
        }
    
    def _chat_no_stream(self, messages: List[Dict[str, Any]], **kwargs) -> str:
        """
        OpenAI非流式聊天实现
        """
        cache_key = self._generate_cache_key(messages, **kwargs)
        cached_result = self._get_from_cache(cache_key)
        if cached_result:
            return cached_result
        
        request_data = {
            "model": self.config.model,
            "messages": messages,
            **(self.config.generate_cfg or {})
        }
        
        # 使用aiohttp进行同步调用
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(self._make_openai_request(request_data))
            self._set_to_cache(cache_key, result)
            return result
        finally:
            loop.close()
    
    def _chat_stream(self, messages: List[Dict[str, Any]], **kwargs) -> Iterator[str]:
        """
        OpenAI流式聊天实现
        """
        request_data = {
            "model": self.config.model,
            "messages": messages,
            "stream": True,
            **(self.config.generate_cfg or {})
        }
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            for chunk in loop.run_until_complete(self._make_openai_stream_request(request_data)):
                yield chunk
        finally:
            loop.close()
    
    async def _make_openai_request(self, data: Dict[str, Any]) -> str:
        """
        OpenAI HTTP请求
        """
        session = await self._get_session()
        
        try:
            async with session.post(self.base_url, headers=self.headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    raise Exception(f"OpenAI request failed: {response.status}")
        except Exception as e:
            print(f"OpenAI request error: {e}")
            raise
    
    async def _make_openai_stream_request(self, data: Dict[str, Any]) -> Iterator[str]:
        """
        OpenAI流式请求
        """
        session = await self._get_session()
        
        try:
            async with session.post(self.base_url, headers=self.headers, json=data) as response:
                if response.status == 200:
                    async for line in response.content:
                        line = line.decode('utf-8').strip()
                        if line.startswith('data: '):
                            data_str = line[6:]
                            if data_str != '[DONE]':
                                try:
                                    data = json.loads(data_str)
                                    if 'choices' in data and len(data['choices']) > 0:
                                        delta = data['choices'][0].get('delta', {})
                                        if 'content' in delta:
                                            yield delta['content']
                                except json.JSONDecodeError:
                                    continue
                else:
                    raise Exception(f"OpenAI stream request failed: {response.status}")
        except Exception as e:
            print(f"OpenAI stream request error: {e}")
            raise

class LLMFactory:
    """
    LLM工厂类
    """
    @staticmethod
    def create_llm(config: LLMConfig) -> BaseLLM:
        """
        根据配置创建LLM实例
        """
        if config.model_type == 'qwen_dashscope':
            return QwenLLM(config)
        elif config.model_type == 'openai':
            return OpenAILLM(config)
        else:
            raise ValueError(f"Unsupported model type: {config.model_type}")

# 使用示例
if __name__ == "__main__":
    # 配置缓存
    redis_client = redis.Redis(host='localhost', port=6379, db=0)
    
    # 创建Qwen配置
    qwen_config = LLMConfig(
        model_type='qwen_dashscope',
        model='qwen-turbo',
        api_key='your_api_key',
        generate_cfg={'temperature': 0.7, 'max_tokens': 1000}
    )
    
    # 创建Qwen LLM
    qwen_llm = LLMFactory.create_llm(qwen_config)
    qwen_llm.set_cache(redis_client)
    
    # 测试非流式聊天
    messages = [
        {"role": "user", "content": "Hello, how are you?"}
    ]
    
    print("Testing non-streaming chat...")
    response = qwen_llm.chat(messages, stream=False)
    print(f"Response: {response}")
    
    # 测试流式聊天
    print("\nTesting streaming chat...")
    for chunk in qwen_llm.chat(messages, stream=True):
        print(f"Chunk: {chunk}", end="", flush=True)
    print()
```

**关键点：**
1. 统一的LLM接口设计
2. 支持多种LLM提供商
3. 流式处理和非流式处理
4. 缓存机制
5. 异步HTTP请求
6. 错误处理和异常管理

---

### 1.3 工具系统实现

**问题4：请实现一个完整的工具注册和调用系统，包含参数验证和错误处理。**

**参考答案：**
```python
import json
import inspect
from typing import Dict, Any, List, Optional, Type, Callable
from abc import ABC, abstractmethod
import functools
import traceback
from pydantic import BaseModel, ValidationError, create_model
from enum import Enum

class ToolStatus(Enum):
    """工具状态枚举"""
    READY = "ready"
    RUNNING = "running"
    ERROR = "error"
    DISABLED = "disabled"

class ToolResult:
    """工具调用结果"""
    def __init__(self, 
                 success: bool,
                 result: Any = None,
                 error: Optional[str] = None,
                 execution_time: float = 0.0):
        self.success = success
        self.result = result
        self.error = error
        self.execution_time = execution_time
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典"""
        return {
            "success": self.success,
            "result": self.result,
            "error": self.error,
            "execution_time": self.execution_time
        }

class BaseTool(ABC):
    """工具基类"""
    
    def __init__(self, name: str, description: str):
        self.name = name
        self.description = description
        self.status = ToolStatus.READY
        self.execution_count = 0
        self.total_execution_time = 0.0
    
    @abstractmethod
    def execute(self, **kwargs) -> ToolResult:
        """执行工具"""
        raise NotImplementedError
    
    def validate_parameters(self, params: Dict[str, Any]) -> bool:
        """验证参数"""
        return True
    
    def get_schema(self) -> Dict[str, Any]:
        """获取工具schema"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": self.get_parameters_schema()
        }
    
    def get_parameters_schema(self) -> Dict[str, Any]:
        """获取参数schema"""
        return {"type": "object", "properties": {}, "required": []}
    
    def pre_execute(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """执行前处理"""
        return params
    
    def post_execute(self, result: ToolResult) -> ToolResult:
        """执行后处理"""
        return result
    
    def __call__(self, **kwargs) -> ToolResult:
        """调用工具"""
        import time
        start_time = time.time()
        
        try:
            # 验证参数
            if not self.validate_parameters(kwargs):
                return ToolResult(
                    success=False,
                    error="Invalid parameters"
                )
            
            # 预处理
            processed_params = self.pre_execute(kwargs)
            
            # 执行
            self.status = ToolStatus.RUNNING
            result = self.execute(**processed_params)
            
            # 后处理
            final_result = self.post_execute(result)
            
            # 更新统计
            self.execution_count += 1
            execution_time = time.time() - start_time
            self.total_execution_time += execution_time
            final_result.execution_time = execution_time
            
            self.status = ToolStatus.READY
            return final_result
            
        except Exception as e:
            self.status = ToolStatus.ERROR
            return ToolResult(
                success=False,
                error=str(e),
                execution_time=time.time() - start_time
            )

class FunctionTool(BaseTool):
    """函数工具"""
    
    def __init__(self, func: Callable, name: str = None, description: str = None):
        if name is None:
            name = func.__name__
        if description is None:
            description = func.__doc__ or f"Function: {name}"
        
        super().__init__(name, description)
        self.func = func
        self.parameters_schema = self._generate_parameters_schema()
    
    def _generate_parameters_schema(self) -> Dict[str, Any]:
        """生成参数schema"""
        sig = inspect.signature(self.func)
        parameters = {}
        required = []
        
        for param_name, param in sig.parameters.items():
            param_type = param.annotation
            if param_type == inspect.Parameter.empty:
                param_type = str
            
            # 简化的类型映射
            type_mapping = {
                str: "string",
                int: "integer",
                float: "number",
                bool: "boolean",
                list: "array",
                dict: "object"
            }
            
            param_schema = {
                "type": type_mapping.get(param_type, "string"),
                "description": f"Parameter: {param_name}"
            }
            
            if param.default == inspect.Parameter.empty:
                required.append(param_name)
            else:
                param_schema["default"] = param.default
            
            parameters[param_name] = param_schema
        
        return {
            "type": "object",
            "properties": parameters,
            "required": required
        }
    
    def validate_parameters(self, params: Dict[str, Any]) -> bool:
        """验证参数"""
        try:
            # 使用Pydantic进行参数验证
            model_fields = {}
            for param_name, param_schema in self.parameters_schema["properties"].items():
                field_type = self._get_python_type(param_schema["type"])
                if param_name in self.parameters_schema.get("required", []):
                    model_fields[param_name] = (field_type, ...)
                else:
                    model_fields[param_name] = (field_type, None)
            
            # 动态创建Pydantic模型
            DynamicModel = create_model('DynamicModel', **model_fields)
            DynamicModel(**params)
            return True
        except ValidationError:
            return False
        except Exception:
            return False
    
    def _get_python_type(self, schema_type: str) -> Type:
        """将schema类型转换为Python类型"""
        type_mapping = {
            "string": str,
            "integer": int,
            "number": float,
            "boolean": bool,
            "array": list,
            "object": dict
        }
        return type_mapping.get(schema_type, str)
    
    def execute(self, **kwargs) -> ToolResult:
        """执行函数"""
        try:
            result = self.func(**kwargs)
            return ToolResult(success=True, result=result)
        except Exception as e:
            return ToolResult(success=False, error=str(e))
    
    def get_parameters_schema(self) -> Dict[str, Any]:
        """获取参数schema"""
        return self.parameters_schema

class ToolRegistry:
    """工具注册中心"""
    
    def __init__(self):
        self.tools: Dict[str, BaseTool] = {}
        self.tool_categories: Dict[str, List[str]] = {}
    
    def register_tool(self, tool: BaseTool, category: str = "general") -> None:
        """注册工具"""
        if tool.name in self.tools:
            raise ValueError(f"Tool '{tool.name}' already registered")
        
        self.tools[tool.name] = tool
        
        if category not in self.tool_categories:
            self.tool_categories[category] = []
        self.tool_categories[category].append(tool.name)
        
        print(f"Tool '{tool.name}' registered in category '{category}'")
    
    def register_function(self, func: Callable, name: str = None, 
                         description: str = None, category: str = "general") -> None:
        """注册函数为工具"""
        tool = FunctionTool(func, name, description)
        self.register_tool(tool, category)
    
    def unregister_tool(self, tool_name: str) -> None:
        """注销工具"""
        if tool_name in self.tools:
            del self.tools[tool_name]
            # 从分类中移除
            for category, tools in self.tool_categories.items():
                if tool_name in tools:
                    tools.remove(tool_name)
            print(f"Tool '{tool_name}' unregistered")
    
    def get_tool(self, tool_name: str) -> Optional[BaseTool]:
        """获取工具"""
        return self.tools.get(tool_name)
    
    def list_tools(self, category: str = None) -> List[str]:
        """列出工具"""
        if category:
            return self.tool_categories.get(category, [])
        return list(self.tools.keys())
    
    def get_tools_schema(self) -> List[Dict[str, Any]]:
        """获取所有工具的schema"""
        return [tool.get_schema() for tool in self.tools.values()]
    
    def call_tool(self, tool_name: str, **kwargs) -> ToolResult:
        """调用工具"""
        tool = self.get_tool(tool_name)
        if not tool:
            return ToolResult(
                success=False,
                error=f"Tool '{tool_name}' not found"
            )
        
        return tool(**kwargs)
    
    def get_tool_stats(self) -> Dict[str, Dict[str, Any]]:
        """获取工具统计信息"""
        stats = {}
        for name, tool in self.tools.items():
            stats[name] = {
                "execution_count": tool.execution_count,
                "total_execution_time": tool.total_execution_time,
                "average_execution_time": (
                    tool.total_execution_time / tool.execution_count
                    if tool.execution_count > 0 else 0
                ),
                "status": tool.status.value
            }
        return stats

def tool(name: str = None, description: str = None, category: str = "general"):
    """工具装饰器"""
    def decorator(func):
        tool_name = name or func.__name__
        tool_description = description or func.__doc__ or f"Function: {tool_name}"
        
        # 这里应该有一个全局的registry实例
        # 为了示例，我们创建一个临时的
        registry = ToolRegistry()
        registry.register_function(func, tool_name, tool_description, category)
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        
        return wrapper
    return decorator

# 示例工具函数
@tool(name="calculator", description="Basic calculator", category="math")
def calculator(operation: str, a: float, b: float) -> float:
    """Basic calculator operations"""
    if operation == "add":
        return a + b
    elif operation == "subtract":
        return a - b
    elif operation == "multiply":
        return a * b
    elif operation == "divide":
        if b == 0:
            raise ValueError("Cannot divide by zero")
        return a / b
    else:
        raise ValueError(f"Unknown operation: {operation}")

@tool(name="text_processor", description="Text processing utilities", category="text")
def text_processor(text: str, operation: str = "upper") -> str:
    """Text processing operations"""
    if operation == "upper":
        return text.upper()
    elif operation == "lower":
        return text.lower()
    elif operation == "reverse":
        return text[::-1]
    elif operation == "word_count":
        return str(len(text.split()))
    else:
        raise ValueError(f"Unknown operation: {operation}")

# 自定义工具类
class WeatherTool(BaseTool):
    def __init__(self):
        super().__init__(
            name="weather",
            description="Get weather information for a city"
        )
    
    def validate_parameters(self, params: Dict[str, Any]) -> bool:
        return "city" in params and isinstance(params["city"], str)
    
    def get_parameters_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "City name to get weather for"
                }
            },
            "required": ["city"]
        }
    
    def execute(self, **kwargs) -> ToolResult:
        city = kwargs["city"]
        # 模拟天气API调用
        weather_data = {
            "Beijing": {"temp": 25, "condition": "sunny"},
            "Shanghai": {"temp": 28, "condition": "cloudy"},
            "Guangzhou": {"temp": 32, "condition": "rainy"}
        }
        
        if city in weather_data:
            data = weather_data[city]
            return ToolResult(
                success=True,
                result=f"Weather in {city}: {data['condition']}, {data['temp']}°C"
            )
        else:
            return ToolResult(
                success=False,
                error=f"Weather data not available for {city}"
            )

# 使用示例
if __name__ == "__main__":
    # 创建工具注册中心
    registry = ToolRegistry()
    
    # 注册自定义工具
    weather_tool = WeatherTool()
    registry.register_tool(weather_tool, "weather")
    
    # 列出所有工具
    print("Available tools:")
    for category in registry.tool_categories:
        print(f"  {category}: {registry.list_tools(category)}")
    
    # 获取工具schema
    print("\nTool schemas:")
    for schema in registry.get_tools_schema():
        print(f"  {schema['name']}: {schema['description']}")
    
    # 测试工具调用
    print("\nTesting tools:")
    
    # 测试计算器
    result = registry.call_tool("calculator", operation="add", a=5, b=3)
    print(f"Calculator result: {result.to_dict()}")
    
    # 测试文本处理
    result = registry.call_tool("text_processor", text="Hello World", operation="upper")
    print(f"Text processor result: {result.to_dict()}")
    
    # 测试天气工具
    result = registry.call_tool("weather", city="Beijing")
    print(f"Weather tool result: {result.to_dict()}")
    
    # 获取工具统计
    print("\nTool statistics:")
    stats = registry.get_tool_stats()
    for tool_name, stat in stats.items():
        print(f"  {tool_name}: {stat}")
```

**关键点：**
1. 工具基类和接口设计
2. 参数验证和类型检查
3. 错误处理和异常管理
4. 工具注册和管理
5. 统计和监控
6. 装饰器支持

---

## 2. 高级功能实现

### 2.1 并发处理

**问题5：请实现一个支持并发处理的消息处理器，包含任务队列和结果收集。**

**参考答案：**
```python
import asyncio
import threading
import time
from typing import List, Dict, Any, Optional, Callable, Iterator
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue, Empty
from dataclasses import dataclass
import uuid
import json

@dataclass
class Task:
    """任务定义"""
    id: str
    func: Callable
    args: tuple
    kwargs: dict
    priority: int = 0
    timeout: Optional[float] = None
    created_at: float = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()

@dataclass
class TaskResult:
    """任务结果"""
    task_id: str
    success: bool
    result: Any = None
    error: Optional[str] = None
    execution_time: float = 0.0
    worker_id: Optional[str] = None

class TaskQueue:
    """任务队列"""
    
    def __init__(self, max_size: int = 1000):
        self.queue = Queue(maxsize=max_size)
        self.priority_queue = []  # 优先级队列
        self.lock = threading.Lock()
    
    def put(self, task: Task):
        """添加任务"""
        with self.lock:
            if task.priority > 0:
                # 高优先级任务
                self.priority_queue.append(task)
                self.priority_queue.sort(key=lambda x: x.priority, reverse=True)
            else:
                # 普通任务
                self.queue.put(task)
    
    def get(self, timeout: Optional[float] = None) -> Optional[Task]:
        """获取任务"""
        with self.lock:
            # 先处理高优先级任务
            if self.priority_queue:
                return self.priority_queue.pop(0)
            
            # 处理普通任务
            try:
                return self.queue.get(timeout=timeout)
            except Empty:
                return None
    
    def size(self) -> int:
        """队列大小"""
        with self.lock:
            return self.queue.qsize() + len(self.priority_queue)
    
    def empty(self) -> bool:
        """队列是否为空"""
        with self.lock:
            return self.queue.empty() and not self.priority_queue

class Worker:
    """工作线程"""
    
    def __init__(self, worker_id: str, task_queue: TaskQueue, result_handler: Callable):
        self.worker_id = worker_id
        self.task_queue = task_queue
        self.result_handler = result_handler
        self.running = False
        self.thread = None
        self.current_task: Optional[Task] = None
    
    def start(self):
        """启动工作线程"""
        self.running = True
        self.thread = threading.Thread(target=self._work_loop, name=f"Worker-{self.worker_id}")
        self.thread.daemon = True
        self.thread.start()
        print(f"Worker {self.worker_id} started")
    
    def stop(self):
        """停止工作线程"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print(f"Worker {self.worker_id} stopped")
    
    def _work_loop(self):
        """工作循环"""
        while self.running:
            try:
                task = self.task_queue.get(timeout=1.0)
                if task:
                    self.current_task = task
                    result = self._execute_task(task)
                    self.result_handler(result)
                    self.current_task = None
            except Exception as e:
                print(f"Worker {self.worker_id} error: {e}")
                time.sleep(1)
    
    def _execute_task(self, task: Task) -> TaskResult:
        """执行任务"""
        start_time = time.time()
        
        try:
            # 执行任务
            if asyncio.iscoroutinefunction(task.func):
                # 异步函数
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(task.func(*task.args, **task.kwargs))
                finally:
                    loop.close()
            else:
                # 同步函数
                result = task.func(*task.args, **task.kwargs)
            
            execution_time = time.time() - start_time
            
            return TaskResult(
                task_id=task.id,
                success=True,
                result=result,
                execution_time=execution_time,
                worker_id=self.worker_id
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            return TaskResult(
                task_id=task.id,
                success=False,
                error=str(e),
                execution_time=execution_time,
                worker_id=self.worker_id
            )

class ConcurrentProcessor:
    """并发处理器"""
    
    def __init__(self, 
                 max_workers: int = 4,
                 max_queue_size: int = 1000,
                 enable_priority: bool = True):
        self.max_workers = max_workers
        self.task_queue = TaskQueue(max_queue_size)
        self.workers: List[Worker] = []
        self.results: Dict[str, TaskResult] = {}
        self.result_lock = threading.Lock()
        self.running = False
        
        # 统计信息
        self.stats = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "total_execution_time": 0.0
        }
    
    def start(self):
        """启动处理器"""
        if self.running:
            return
        
        self.running = True
        
        # 创建工作线程
        for i in range(self.max_workers):
            worker = Worker(
                worker_id=str(i),
                task_queue=self.task_queue,
                result_handler=self._handle_result
            )
            worker.start()
            self.workers.append(worker)
        
        print(f"ConcurrentProcessor started with {self.max_workers} workers")
    
    def stop(self):
        """停止处理器"""
        if not self.running:
            return
        
        self.running = False
        
        # 停止所有工作线程
        for worker in self.workers:
            worker.stop()
        
        self.workers.clear()
        print("ConcurrentProcessor stopped")
    
    def submit_task(self, 
                   func: Callable,
                   args: tuple = (),
                   kwargs: dict = None,
                   priority: int = 0,
                   timeout: Optional[float] = None) -> str:
        """提交任务"""
        if not self.running:
            raise RuntimeError("Processor not running")
        
        task = Task(
            id=str(uuid.uuid4()),
            func=func,
            args=args,
            kwargs=kwargs or {},
            priority=priority,
            timeout=timeout
        )
        
        self.task_queue.put(task)
        
        with self.result_lock:
            self.stats["total_tasks"] += 1
        
        return task.id
    
    def submit_tasks(self, tasks: List[Dict[str, Any]]) -> List[str]:
        """批量提交任务"""
        task_ids = []
        for task_config in tasks:
            task_id = self.submit_task(**task_config)
            task_ids.append(task_id)
        return task_ids
    
    def get_result(self, task_id: str, timeout: Optional[float] = None) -> Optional[TaskResult]:
        """获取任务结果"""
        start_time = time.time()
        
        while True:
            with self.result_lock:
                if task_id in self.results:
                    return self.results[task_id]
            
            if timeout is not None:
                elapsed = time.time() - start_time
                if elapsed >= timeout:
                    return None
                time.sleep(0.1)
            else:
                time.sleep(0.1)
    
    def get_results(self, task_ids: List[str], timeout: Optional[float] = None) -> Dict[str, TaskResult]:
        """批量获取任务结果"""
        results = {}
        
        for task_id in task_ids:
            result = self.get_result(task_id, timeout)
            if result:
                results[task_id] = result
        
        return results
    
    def wait_for_completion(self, task_ids: List[str], timeout: Optional[float] = None) -> bool:
        """等待任务完成"""
        start_time = time.time()
        
        while True:
            with self.result_lock:
                completed = all(task_id in self.results for task_id in task_ids)
            
            if completed:
                return True
            
            if timeout is not None:
                elapsed = time.time() - start_time
                if elapsed >= timeout:
                    return False
                time.sleep(0.1)
            else:
                time.sleep(0.1)
    
    def _handle_result(self, result: TaskResult):
        """处理任务结果"""
        with self.result_lock:
            self.results[result.task_id] = result
            
            # 更新统计
            self.stats["completed_tasks"] += 1
            self.stats["total_execution_time"] += result.execution_time
            
            if not result.success:
                self.stats["failed_tasks"] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """获取统计信息"""
        with self.result_lock:
            stats = self.stats.copy()
            stats["queue_size"] = self.task_queue.size()
            stats["active_workers"] = len([w for w in self.workers if w.current_task is not None])
            stats["average_execution_time"] = (
                stats["total_execution_time"] / stats["completed_tasks"]
                if stats["completed_tasks"] > 0 else 0
            )
            stats["success_rate"] = (
                (stats["completed_tasks"] - stats["failed_tasks"]) / stats["completed_tasks"]
                if stats["completed_tasks"] > 0 else 0
            )
            return stats
    
    def clear_results(self):
        """清理结果"""
        with self.result_lock:
            self.results.clear()
    
    def get_worker_status(self) -> List[Dict[str, Any]]:
        """获取工作线程状态"""
        return [
            {
                "worker_id": worker.worker_id,
                "current_task": worker.current_task.id if worker.current_task else None,
                "running": worker.running
            }
            for worker in self.workers
        ]

# 示例任务函数
def cpu_intensive_task(n: int) -> int:
    """CPU密集型任务"""
    result = 0
    for i in range(n):
        result += i * i
    return result

def io_intensive_task(url: str) -> str:
    """IO密集型任务"""
    import time
    time.sleep(1)  # 模拟网络请求
    return f"Response from {url}"

async def async_task(message: str) -> str:
    """异步任务"""
    await asyncio.sleep(0.5)
    return f"Async result: {message}"

def failing_task() -> str:
    """失败任务"""
    raise ValueError("This task always fails")

# 使用示例
if __name__ == "__main__":
    # 创建并发处理器
    processor = ConcurrentProcessor(max_workers=4)
    processor.start()
    
    try:
        # 提交各种任务
        task_ids = []
        
        # CPU密集型任务
        task_ids.append(processor.submit_task(cpu_intensive_task, args=(1000000,)))
        
        # IO密集型任务
        task_ids.append(processor.submit_task(io_intensive_task, kwargs={"url": "https://example.com"}))
        
        # 异步任务
        task_ids.append(processor.submit_task(async_task, kwargs={"message": "Hello"}))
        
        # 失败任务
        task_ids.append(processor.submit_task(failing_task))
        
        # 高优先级任务
        task_ids.append(processor.submit_task(
            cpu_intensive_task, 
            args=(100000,), 
            priority=10
        ))
        
        # 批量任务
        batch_tasks = [
            {"func": cpu_intensive_task, "args": (100000,)},
            {"func": io_intensive_task, "kwargs": {"url": "https://test.com"}},
            {"func": async_task, "kwargs": {"message": "Batch"}}
        ]
        batch_ids = processor.submit_tasks(batch_tasks)
        task_ids.extend(batch_ids)
        
        print(f"Submitted {len(task_ids)} tasks")
        
        # 等待任务完成
        processor.wait_for_completion(task_ids, timeout=30)
        
        # 获取结果
        results = processor.get_results(task_ids)
        
        print("\nTask Results:")
        for task_id, result in results.items():
            status = "SUCCESS" if result.success else "FAILED"
            print(f"  {task_id[:8]}...: {status} ({result.execution_time:.3f}s)")
            if result.success:
                print(f"    Result: {result.result}")
            else:
                print(f"    Error: {result.error}")
        
        # 显示统计信息
        print("\nProcessor Statistics:")
        stats = processor.get_stats()
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.3f}")
            else:
                print(f"  {key}: {value}")
        
        # 显示工作线程状态
        print("\nWorker Status:")
        for worker_status in processor.get_worker_status():
            print(f"  Worker {worker_status['worker_id']}: "
                  f"{'Busy' if worker_status['current_task'] else 'Idle'}")
    
    finally:
        processor.stop()
```

**关键点：**
1. 任务队列和工作线程池
2. 优先级任务处理
3. 异步和同步任务支持
4. 结果收集和统计
5. 错误处理和异常管理
6. 资源管理和清理

---

### 2.2 缓存系统

**问题6：请实现一个多级缓存系统，包含内存缓存、Redis缓存和缓存失效策略。**

**参考答案：**
```python
import time
import json
import threading
from typing import Any, Optional, Dict, List, Callable, Union
from abc import ABC, abstractmethod
from dataclasses import dataclass
import hashlib
import pickle
import redis
from functools import wraps

@dataclass
class CacheEntry:
    """缓存条目"""
    key: str
    value: Any
    created_at: float
    ttl: Optional[float] = None
    access_count: int = 0
    last_accessed: float = None
    
    def __post_init__(self):
        if self.last_accessed is None:
            self.last_accessed = self.created_at
    
    def is_expired(self) -> bool:
        """检查是否过期"""
        if self.ttl is None:
            return False
        return time.time() - self.created_at > self.ttl
    
    def record_access(self):
        """记录访问"""
        self.access_count += 1
        self.last_accessed = time.time()

class BaseCache(ABC):
    """缓存基类"""
    
    def __init__(self, default_ttl: Optional[float] = None):
        self.default_ttl = default_ttl
    
    @abstractmethod
    def get(self, key: str) -> Optional[Any]:
        """获取缓存"""
        pass
    
    @abstractmethod
    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        """设置缓存"""
        pass
    
    @abstractmethod
    def delete(self, key: str) -> bool:
        """删除缓存"""
        pass
    
    @abstractmethod
    def clear(self) -> bool:
        """清空缓存"""
        pass
    
    @abstractmethod
    def exists(self, key: str) -> bool:
        """检查键是否存在"""
        pass
    
    def generate_key(self, *args, **kwargs) -> str:
        """生成缓存键"""
        key_data = {
            "args": args,
            "kwargs": kwargs
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()

class MemoryCache(BaseCache):
    """内存缓存"""
    
    def __init__(self, max_size: int = 1000, default_ttl: Optional[float] = None):
        super().__init__(default_ttl)
        self.max_size = max_size
        self.cache: Dict[str, CacheEntry] = {}
        self.lock = threading.RLock()
        self.cleanup_interval = 60  # 清理间隔（秒）
        self.last_cleanup = time.time()
    
    def get(self, key: str) -> Optional[Any]:
        """获取缓存"""
        with self.lock:
            # 定期清理过期缓存
            self._cleanup_if_needed()
            
            if key in self.cache:
                entry = self.cache[key]
                if entry.is_expired():
                    del self.cache[key]
                    return None
                
                entry.record_access()
                return entry.value
            
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        """设置缓存"""
        with self.lock:
            # 如果缓存已满，清理空间
            if len(self.cache) >= self.max_size:
                self._evict_lru()
            
            # 使用默认TTL如果未指定
            if ttl is None:
                ttl = self.default_ttl
            
            entry = CacheEntry(
                key=key,
                value=value,
                created_at=time.time(),
                ttl=ttl
            )
            
            self.cache[key] = entry
            return True
    
    def delete(self, key: str) -> bool:
        """删除缓存"""
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                return True
            return False
    
    def clear(self) -> bool:
        """清空缓存"""
        with self.lock:
            self.cache.clear()
            return True
    
    def exists(self, key: str) -> bool:
        """检查键是否存在"""
        with self.lock:
            return key in self.cache and not self.cache[key].is_expired()
    
    def _cleanup_if_needed(self):
        """如果需要则清理缓存"""
        current_time = time.time()
        if current_time - self.last_cleanup > self.cleanup_interval:
            self._cleanup_expired()
            self.last_cleanup = current_time
    
    def _cleanup_expired(self):
        """清理过期缓存"""
        expired_keys = []
        for key, entry in self.cache.items():
            if entry.is_expired():
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
    
    def _evict_lru(self):
        """LRU淘汰策略"""
        if not self.cache:
            return
        
        # 找到最久未使用的条目
        lru_key = min(self.cache.keys(), 
                     key=lambda k: self.cache[k].last_accessed)
        
        del self.cache[lru_key]
    
    def get_stats(self) -> Dict[str, Any]:
        """获取缓存统计"""
        with self.lock:
            total_entries = len(self.cache)
            expired_count = sum(1 for entry in self.cache.values() if entry.is_expired())
            total_accesses = sum(entry.access_count for entry in self.cache.values())
            
            return {
                "total_entries": total_entries,
                "expired_entries": expired_count,
                "active_entries": total_entries - expired_count,
                "total_accesses": total_accesses,
                "hit_rate": self._calculate_hit_rate()
            }
    
    def _calculate_hit_rate(self) -> float:
        """计算命中率（简化版）"""
        # 这里应该维护hit/miss计数器
        return 0.0

class RedisCache(BaseCache):
    """Redis缓存"""
    
    def __init__(self, 
                 host: str = 'localhost',
                 port: int = 6379,
                 db: int = 0,
                 password: Optional[str] = None,
                 default_ttl: Optional[float] = None,
                 key_prefix: str = "cache:"):
        super().__init__(default_ttl)
        self.key_prefix = key_prefix
        self.client = redis.Redis(
            host=host,
            port=port,
            db=db,
            password=password,
            decode_responses=False  # 使用二进制模式
        )
    
    def _get_full_key(self, key: str) -> str:
        """获取完整的Redis键"""
        return f"{self.key_prefix}{key}"
    
    def get(self, key: str) -> Optional[Any]:
        """获取缓存"""
        try:
            full_key = self._get_full_key(key)
            data = self.client.get(full_key)
            if data:
                return pickle.loads(data)
            return None
        except Exception as e:
            print(f"Redis get error: {e}")
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        """设置缓存"""
        try:
            full_key = self._get_full_key(key)
            data = pickle.dumps(value)
            
            if ttl is None:
                ttl = self.default_ttl
            
            if ttl:
                return self.client.setex(full_key, int(ttl), data)
            else:
                return self.client.set(full_key, data)
        except Exception as e:
            print(f"Redis set error: {e}")
            return False
    
    def delete(self, key: str) -> bool:
        """删除缓存"""
        try:
            full_key = self._get_full_key(key)
            return bool(self.client.delete(full_key))
        except Exception as e:
            print(f"Redis delete error: {e}")
            return False
    
    def clear(self) -> bool:
        """清空缓存"""
        try:
            pattern = self._get_full_key("*")
            keys = self.client.keys(pattern)
            if keys:
                return bool(self.client.delete(*keys))
            return True
        except Exception as e:
            print(f"Redis clear error: {e}")
            return False
    
    def exists(self, key: str) -> bool:
        """检查键是否存在"""
        try:
            full_key = self._get_full_key(key)
            return bool(self.client.exists(full_key))
        except Exception as e:
            print(f"Redis exists error: {e}")
            return False

class MultiLevelCache:
    """多级缓存"""
    
    def __init__(self, 
                 caches: List[BaseCache],
                 write_through: bool = True,
                 write_back: bool = False):
        self.caches = caches
        self.write_through = write_through
        self.write_back = write_back
        self.lock = threading.RLock()
        
        # 统计信息
        self.stats = {
            "hits": 0,
            "misses": 0,
            "level_hits": [0] * len(caches)
        }
    
    def get(self, key: str) -> Optional[Any]:
        """获取缓存（多级查找）"""
        with self.lock:
            # 从第一级缓存开始查找
            for i, cache in enumerate(self.caches):
                try:
                    value = cache.get(key)
                    if value is not None:
                        # 命中统计
                        self.stats["hits"] += 1
                        self.stats["level_hits"][i] += 1
                        
                        # 回填到上级缓存
                        self._populate_upper_caches(key, value, i)
                        
                        return value
                except Exception as e:
                    print(f"Cache level {i} get error: {e}")
            
            # 未命中
            self.stats["misses"] += 1
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[float] = None) -> bool:
        """设置缓存"""
        with self.lock:
            success = True
            
            if self.write_through:
                # 写透模式：写入所有缓存
                for cache in self.caches:
                    try:
                        if not cache.set(key, value, ttl):
                            success = False
                    except Exception as e:
                        print(f"Cache set error: {e}")
                        success = False
            else:
                # 只写入第一级缓存
                if self.caches:
                    try:
                        success = self.caches[0].set(key, value, ttl)
                    except Exception as e:
                        print(f"Cache set error: {e}")
                        success = False
            
            return success
    
    def delete(self, key: str) -> bool:
        """删除缓存"""
        with self.lock:
            success = True
            for cache in self.caches:
                try:
                    if not cache.delete(key):
                        success = False
                except Exception as e:
                    print(f"Cache delete error: {e}")
                    success = False
            return success
    
    def clear(self) -> bool:
        """清空缓存"""
        with self.lock:
            success = True
            for cache in self.caches:
                try:
                    if not cache.clear():
                        success = False
                except Exception as e:
                    print(f"Cache clear error: {e}")
                    success = False
            return success
    
    def exists(self, key: str) -> bool:
        """检查键是否存在"""
        with self.lock:
            for cache in self.caches:
                try:
                    if cache.exists(key):
                        return True
                except Exception as e:
                    print(f"Cache exists error: {e}")
            return False
    
    def _populate_upper_caches(self, key: str, value: Any, from_level: int):
        """回填到上级缓存"""
        for i in range(from_level):
            try:
                self.caches[i].set(key, value)
            except Exception as e:
                print(f"Cache populate error: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """获取统计信息"""
        with self.lock:
            total_requests = self.stats["hits"] + self.stats["misses"]
            hit_rate = self.stats["hits"] / total_requests if total_requests > 0 else 0
            
            return {
                "total_requests": total_requests,
                "hits": self.stats["hits"],
                "misses": self.stats["misses"],
                "hit_rate": hit_rate,
                "level_hits": self.stats["level_hits"],
                "cache_count": len(self.caches)
            }

def cached(cache: MultiLevelCache, 
           key_func: Optional[Callable] = None,
           ttl: Optional[float] = None):
    """缓存装饰器"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # 生成缓存键
            if key_func:
                key = key_func(*args, **kwargs)
            else:
                key = cache.generate_key(func.__name__, args, kwargs)
            
            # 尝试从缓存获取
            result = cache.get(key)
            if result is not None:
                return result
            
            # 执行函数并缓存结果
            result = func(*args, **kwargs)
            cache.set(key, result, ttl)
            return result
        
        return wrapper
    return decorator

# 使用示例
if __name__ == "__main__":
    # 创建多级缓存
    memory_cache = MemoryCache(max_size=1000, default_ttl=300)
    redis_cache = RedisCache(host='localhost', port=6379, default_ttl=3600)
    
    multi_cache = MultiLevelCache(
        caches=[memory_cache, redis_cache],
        write_through=True
    )
    
    # 测试基本操作
    print("Testing basic cache operations:")
    
    # 设置缓存
    multi_cache.set("key1", "value1")
    multi_cache.set("key2", {"data": "complex_value"}, ttl=60)
    
    # 获取缓存
    print(f"key1: {multi_cache.get('key1')}")
    print(f"key2: {multi_cache.get('key2')}")
    print(f"key3: {multi_cache.get('key3')}")
    
    # 删除缓存
    multi_cache.delete("key1")
    print(f"key1 after delete: {multi_cache.get('key1')}")
    
    # 测试缓存装饰器
    @cached(multi_cache, ttl=60)
    def expensive_function(x: int, y: int) -> int:
        print(f"Computing expensive_function({x}, {y})")
        import time
        time.sleep(1)  # 模拟耗时操作
        return x * y
    
    print("\nTesting cache decorator:")
    
    # 第一次调用（会执行函数）
    start_time = time.time()
    result1 = expensive_function(5, 3)
    print(f"Result: {result1}, Time: {time.time() - start_time:.3f}s")
    
    # 第二次调用（从缓存获取）
    start_time = time.time()
    result2 = expensive_function(5, 3)
    print(f"Result: {result2}, Time: {time.time() - start_time:.3f}s")
    
    # 显示统计信息
    print("\nCache Statistics:")
    stats = multi_cache.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # 显示各级缓存统计
    print("\nMemory Cache Statistics:")
    mem_stats = memory_cache.get_stats()
    for key, value in mem_stats.items():
        print(f"  {key}: {value}")
```

**关键点：**
1. 多级缓存架构
2. LRU淘汰策略
3. 缓存回填和写透
4. 缓存装饰器
5. 统计和监控
6. 异常处理和错误恢复

---

## 3. 实战问题

### 3.1 系统集成

**问题7：请实现一个完整的Agent系统，集成LLM、工具、缓存和并发处理功能。**

**参考答案：**
```python
import asyncio
import json
import time
from typing import List, Dict, Any, Optional, Iterator, Union
from dataclasses import dataclass, asdict
from enum import Enum
import threading
import uuid

# 导入之前实现的组件
# from concurrent_processor import ConcurrentProcessor, Task, TaskResult
# from multi_level_cache import MultiLevelCache, MemoryCache, RedisCache
# from tool_system import ToolRegistry, BaseTool, ToolResult
# from llm_interface import BaseLLM, LLMFactory, LLMConfig

class AgentState(Enum):
    """Agent状态"""
    IDLE = "idle"
    THINKING = "thinking"
    EXECUTING = "executing"
    RESPONDING = "responding"
    ERROR = "error"

@dataclass
class AgentMessage:
    """Agent消息"""
    id: str
    role: str
    content: str
    timestamp: float
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

@dataclass
class AgentContext:
    """Agent上下文"""
    conversation_id: str
    user_id: str
    session_id: str
    messages: List[AgentMessage]
    variables: Dict[str, Any]
    created_at: float
    last_updated: float
    
    def add_message(self, message: AgentMessage):
        """添加消息"""
        self.messages.append(message)
        self.last_updated = time.time()
    
    def get_recent_messages(self, limit: int = 10) -> List[AgentMessage]:
        """获取最近的消息"""
        return self.messages[-limit:]

class CompleteAgent:
    """完整的Agent系统"""
    
    def __init__(self,
                 name: str,
                 description: str,
                 llm_config: LLMConfig,
                 tool_registry: ToolRegistry,
                 cache: Optional[MultiLevelCache] = None,
                 max_workers: int = 4,
                 system_message: Optional[str] = None):
        self.name = name
        self.description = description
        self.llm_config = llm_config
        self.tool_registry = tool_registry
        self.cache = cache
        self.system_message = system_message
        
        # 初始化组件
        self.llm = LLMFactory.create_llm(llm_config)
        if cache:
            self.llm.set_cache(cache)
        
        self.processor = ConcurrentProcessor(max_workers=max_workers)
        
        # 状态管理
        self.state = AgentState.IDLE
        self.contexts: Dict[str, AgentContext] = {}
        self.context_lock = threading.RLock()
        
        # 统计信息
        self.stats = {
            "total_requests": 0,
            "successful_responses": 0,
            "failed_responses": 0,
            "total_execution_time": 0.0,
            "tool_calls": 0,
            "cache_hits": 0
        }
        
        # 启动处理器
        self.processor.start()
    
    def create_context(self, 
                      conversation_id: str,
                      user_id: str,
                      session_id: str) -> str:
        """创建对话上下文"""
        with self.context_lock:
            context = AgentContext(
                conversation_id=conversation_id,
                user_id=user_id,
                session_id=session_id,
                messages=[],
                variables={},
                created_at=time.time(),
                last_updated=time.time()
            )
            self.contexts[conversation_id] = context
            return conversation_id
    
    def get_context(self, conversation_id: str) -> Optional[AgentContext]:
        """获取对话上下文"""
        with self.context_lock:
            return self.contexts.get(conversation_id)
    
    def delete_context(self, conversation_id: str) -> bool:
        """删除对话上下文"""
        with self.context_lock:
            if conversation_id in self.contexts:
                del self.contexts[conversation_id]
                return True
            return False
    
    def process_message(self,
                       message: str,
                       conversation_id: str,
                       user_id: str,
                       session_id: str,
                       stream: bool = False) -> Union[str, Iterator[str]]:
        """处理用户消息"""
        start_time = time.time()
        
        try:
            # 创建或获取上下文
            context = self.get_context(conversation_id)
            if not context:
                context_id = self.create_context(conversation_id, user_id, session_id)
                context = self.contexts[context_id]
            
            # 创建用户消息
            user_message = AgentMessage(
                id=str(uuid.uuid4()),
                role="user",
                content=message,
                timestamp=time.time()
            )
            context.add_message(user_message)
            
            # 更新统计
            self.stats["total_requests"] += 1
            self.state = AgentState.THINKING
            
            if stream:
                return self._process_streaming(context, user_message)
            else:
                return self._process_non_streaming(context, user_message)
                
        except Exception as e:
            self.state = AgentState.ERROR
            self.stats["failed_responses"] += 1
            raise Exception(f"Message processing failed: {e}")
        
        finally:
            execution_time = time.time() - start_time
            self.stats["total_execution_time"] += execution_time
    
    def _process_non_streaming(self, context: AgentContext, user_message: AgentMessage) -> str:
        """非流式处理"""
        # 1. 生成LLM响应
        self.state = AgentState.THINKING
        llm_response = self._generate_llm_response(context)
        
        # 2. 解析工具调用
        tool_calls = self._parse_tool_calls(llm_response)
        
        if tool_calls:
            # 3. 执行工具调用
            self.state = AgentState.EXECUTING
            tool_results = self._execute_tool_calls(tool_calls)
            
            # 4. 整合结果并生成最终响应
            self.state = AgentState.RESPONDING
            final_response = self._generate_final_response(context, llm_response, tool_results)
        else:
            final_response = llm_response
        
        # 5. 添加助手响应到上下文
        assistant_message = AgentMessage(
            id=str(uuid.uuid4()),
            role="assistant",
            content=final_response,
            timestamp=time.time()
        )
        context.add_message(assistant_message)
        
        # 更新统计
        self.stats["successful_responses"] += 1
        if tool_calls:
            self.stats["tool_calls"] += len(tool_calls)
        
        self.state = AgentState.IDLE
        return final_response
    
    def _process_streaming(self, context: AgentContext, user_message: AgentMessage) -> Iterator[str]:
        """流式处理"""
        # 1. 生成LLM响应（流式）
        self.state = AgentState.THINKING
        llm_response_chunks = []
        
        for chunk in self.llm.chat(
            self._format_messages_for_llm(context),
            stream=True
        ):
            llm_response_chunks.append(chunk)
            yield chunk
        
        llm_response = ''.join(llm_response_chunks)
        
        # 2. 解析工具调用
        tool_calls = self._parse_tool_calls(llm_response)
        
        if tool_calls:
            # 3. 执行工具调用
            self.state = AgentState.EXECUTING
            tool_results = self._execute_tool_calls(tool_calls)
            
            # 4. 生成最终响应（流式）
            self.state = AgentState.RESPONDING
            for chunk in self._generate_final_response_streaming(context, llm_response, tool_results):
                yield chunk
        
        # 5. 添加助手响应到上下文
        assistant_message = AgentMessage(
            id=str(uuid.uuid4()),
            role="assistant",
            content=llm_response,
            timestamp=time.time()
        )
        context.add_message(assistant_message)
        
        # 更新统计
        self.stats["successful_responses"] += 1
        if tool_calls:
            self.stats["tool_calls"] += len(tool_calls)
        
        self.state = AgentState.IDLE
    
    def _format_messages_for_llm(self, context: AgentContext) -> List[Dict[str, Any]]:
        """格式化消息供LLM使用"""
        messages = []
        
        # 添加系统消息
        if self.system_message:
            messages.append({
                "role": "system",
                "content": self.system_message
            })
        
        # 添加对话历史
        for msg in context.get_recent_messages():
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        return messages
    
    def _generate_llm_response(self, context: AgentContext) -> str:
        """生成LLM响应"""
        messages = self._format_messages_for_llm(context)
        
        # 添加工具信息
        tools_schema = self.tool_registry.get_tools_schema()
        if tools_schema:
            messages.append({
                "role": "system",
                "content": f"Available tools: {json.dumps(tools_schema, indent=2)}"
            })
        
        return self.llm.chat(messages, stream=False)
    
    def _parse_tool_calls(self, response: str) -> List[Dict[str, Any]]:
        """解析工具调用"""
        # 简化的工具调用解析
        # 实际实现应该使用更复杂的解析逻辑
        tool_calls = []
        
        # 查找工具调用模式
        import re
        pattern = r'CALL\s+(\w+)\s*\((.*?)\)'
        matches = re.findall(pattern, response, re.DOTALL)
        
        for tool_name, args_str in matches:
            try:
                # 解析参数
                args = {}
                if args_str.strip():
                    # 简单的参数解析
                    for arg_pair in args_str.split(','):
                        if '=' in arg_pair:
                            key, value = arg_pair.split('=', 1)
                            args[key.strip()] = value.strip().strip('"\'')
                
                tool_calls.append({
                    "name": tool_name,
                    "arguments": args
                })
            except Exception as e:
                print(f"Failed to parse tool call: {e}")
        
        return tool_calls
    
    def _execute_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> List[ToolResult]:
        """执行工具调用"""
        results = []
        
        # 提交任务到处理器
        task_ids = []
        for call in tool_calls:
            task_id = self.processor.submit_task(
                func=self.tool_registry.call_tool,
                kwargs={
                    "tool_name": call["name"],
                    **call["arguments"]
                }
            )
            task_ids.append(task_id)
        
        # 等待结果
        self.processor.wait_for_completion(task_ids, timeout=30)
        
        # 收集结果
        for task_id in task_ids:
            result = self.processor.get_result(task_id)
            if result:
                tool_result = ToolResult(
                    success=result.success,
                    result=result.result,
                    error=result.error,
                    execution_time=result.execution_time
                )
                results.append(tool_result)
        
        return results
    
    def _generate_final_response(self, 
                                context: AgentContext, 
                                llm_response: str, 
                                tool_results: List[ToolResult]) -> str:
        """生成最终响应"""
        # 构建包含工具结果的提示
        tool_info = []
        for i, result in enumerate(tool_results):
            status = "SUCCESS" if result.success else "FAILED"
            tool_info.append(f"Tool {i+1}: {status}")
            if result.success:
                tool_info.append(f"Result: {result.result}")
            else:
                tool_info.append(f"Error: {result.error}")
        
        # 再次调用LLM生成最终响应
        messages = self._format_messages_for_llm(context)
        messages.append({
            "role": "assistant",
            "content": llm_response
        })
        messages.append({
            "role": "system",
            "content": f"Tool execution results:\n" + "\n".join(tool_info)
        })
        
        return self.llm.chat(messages, stream=False)
    
    def _generate_final_response_streaming(self, 
                                          context: AgentContext, 
                                          llm_response: str, 
                                          tool_results: List[ToolResult]) -> Iterator[str]:
        """生成最终响应（流式）"""
        # 构建包含工具结果的提示
        tool_info = []
        for i, result in enumerate(tool_results):
            status = "SUCCESS" if result.success else "FAILED"
            tool_info.append(f"Tool {i+1}: {status}")
            if result.success:
                tool_info.append(f"Result: {result.result}")
            else:
                tool_info.append(f"Error: {result.error}")
        
        # 再次调用LLM生成最终响应
        messages = self._format_messages_for_llm(context)
        messages.append({
            "role": "assistant",
            "content": llm_response
        })
        messages.append({
            "role": "system",
            "content": f"Tool execution results:\n" + "\n".join(tool_info)
        })
        
        for chunk in self.llm.chat(messages, stream=True):
            yield chunk
    
    def get_stats(self) -> Dict[str, Any]:
        """获取Agent统计信息"""
        return {
            **self.stats,
            "state": self.state.value,
            "active_contexts": len(self.contexts),
            "processor_stats": self.processor.get_stats()
        }
    
    def cleanup(self):
        """清理资源"""
        self.processor.stop()
        if hasattr(self.llm, 'session') and self.llm.session:
            self.llm.session.close()

# 使用示例
if __name__ == "__main__":
    # 创建组件
    llm_config = LLMConfig(
        model_type='qwen_dashscope',
        model='qwen-turbo',
        api_key='your_api_key',
        generate_cfg={'temperature': 0.7}
    )
    
    tool_registry = ToolRegistry()
    
    # 添加一些示例工具
    def calculator(operation: str, a: float, b: float) -> float:
        if operation == "add":
            return a + b
        elif operation == "subtract":
            return a - b
        elif operation == "multiply":
            return a * b
        elif operation == "divide":
            return a / b if b != 0 else 0
        else:
            raise ValueError(f"Unknown operation: {operation}")
    
    tool_registry.register_function(calculator, "calculator", "Basic calculator")
    
    def get_time() -> str:
        import datetime
        return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    tool_registry.register_function(get_time, "get_time", "Get current time")
    
    # 创建缓存
    memory_cache = MemoryCache(max_size=1000)
    multi_cache = MultiLevelCache(caches=[memory_cache])
    
    # 创建Agent
    agent = CompleteAgent(
        name="complete_agent",
        description="A complete agent with all features",
        llm_config=llm_config,
        tool_registry=tool_registry,
        cache=multi_cache,
        max_workers=4,
        system_message="You are a helpful assistant with access to various tools."
    )
    
    try:
        # 测试Agent
        print("Testing Complete Agent:")
        
        # 创建对话上下文
        conversation_id = agent.create_context("conv1", "user1", "session1")
        
        # 测试消息处理
        response = agent.process_message(
            "Hello! What's 5 + 3?",
            conversation_id,
            "user1",
            "session1"
        )
        print(f"Response: {response}")
        
        # 测试工具调用
        response = agent.process_message(
            "Please calculate 10 * 15 for me",
            conversation_id,
            "user1",
            "session1"
        )
        print(f"Response: {response}")
        
        # 测试流式响应
        print("\nTesting streaming response:")
        for chunk in agent.process_message(
            "Tell me about yourself",
            conversation_id,
            "user1",
            "session1",
            stream=True
        ):
            print(chunk, end="", flush=True)
        print()
        
        # 显示统计信息
        print("\nAgent Statistics:")
        stats = agent.get_stats()
        for key, value in stats.items():
            if isinstance(value, float):
                print(f"  {key}: {value:.3f}")
            else:
                print(f"  {key}: {value}")
    
    finally:
        agent.cleanup()
```

**关键点：**
1. 完整的Agent系统架构
2. 上下文管理和状态维护
3. 工具调用和结果整合
4. 流式和非流式处理
5. 统计和监控
6. 资源管理和清理

---

## 4. 总结

### 4.1 实现要点

**核心实现原则：**

1. **模块化设计**：将系统分解为独立的模块，每个模块负责特定功能
2. **接口抽象**：使用抽象基类定义统一接口，支持多种实现
3. **异步处理**：支持异步操作，提高系统性能
4. **错误处理**：完善的错误处理和异常管理机制
5. **性能优化**：缓存、并发、资源池等优化策略
6. **可扩展性**：支持插件化扩展和配置驱动

### 4.2 技术要点

**关键技术点：**

1. **设计模式**：
   - 工厂模式：LLM和工具的创建
   - 模板方法模式：Agent的处理流程
   - 观察者模式：事件处理和通知
   - 装饰器模式：功能增强

2. **并发处理**：
   - 线程池和任务队列
   - 异步编程
   - 锁和同步机制
   - 资源管理

3. **缓存策略**：
   - 多级缓存
   - LRU淘汰
   - 缓存回填
   - 缓存失效

4. **工具系统**：
   - 动态注册
   - 参数验证
   - 并发执行
   - 结果处理

这些实现展示了如何构建一个完整的AI Agent系统，包含了现代软件工程的最佳实践和设计模式。