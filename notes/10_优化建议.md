# Qwen-Agent 优化建议

## 1. 性能优化

### 1.1 LLM调用优化

**问题分析：**
Qwen-Agent在频繁调用LLM时可能面临以下性能问题：
- API调用延迟高
- 重复请求浪费资源
- 并发处理能力有限
- 成本控制困难

**优化方案：**

1. **智能缓存策略**
```python
import hashlib
import json
import time
from typing import Optional, Dict, Any
from functools import wraps

class LLMCache:
    def __init__(self, redis_client, ttl: int = 3600):
        self.redis = redis_client
        self.ttl = ttl
        self.stats = {"hits": 0, "misses": 0}
    
    def generate_key(self, messages: list, **kwargs) -> str:
        """生成智能缓存键"""
        cache_data = {
            "messages": self._normalize_messages(messages),
            "model": kwargs.get("model", "default"),
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 1000)
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()
    
    def _normalize_messages(self, messages: list) -> list:
        """标准化消息格式"""
        normalized = []
        for msg in messages:
            normalized_msg = {
                "role": msg["role"],
                "content": msg["content"]
            }
            # 可选字段标准化
            if "name" in msg:
                normalized_msg["name"] = msg["name"]
            normalized.append(normalized_msg)
        return normalized
    
    async def get(self, key: str) -> Optional[str]:
        """获取缓存"""
        try:
            result = await self.redis.get(key)
            if result:
                self.stats["hits"] += 1
                return result.decode()
            self.stats["misses"] += 1
            return None
        except Exception as e:
            print(f"Cache get error: {e}")
            return None
    
    async def set(self, key: str, value: str, ttl: Optional[int] = None):
        """设置缓存"""
        try:
            await self.redis.setex(key, ttl or self.ttl, value)
        except Exception as e:
            print(f"Cache set error: {e}")
    
    def get_hit_rate(self) -> float:
        """获取缓存命中率"""
        total = self.stats["hits"] + self.stats["misses"]
        return self.stats["hits"] / total if total > 0 else 0

def cached_llm_call(cache: LLMCache):
    """LLM调用缓存装饰器"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # 提取消息和参数
            messages = kwargs.get("messages", args[0] if args else [])
            cache_key = cache.generate_key(messages, **kwargs)
            
            # 尝试从缓存获取
            cached_result = await cache.get(cache_key)
            if cached_result:
                return cached_result
            
            # 调用原始函数
            result = await func(*args, **kwargs)
            
            # 缓存结果
            await cache.set(cache_key, result)
            
            return result
        return wrapper
    return decorator

# 使用示例
class OptimizedLLM:
    def __init__(self, redis_client):
        self.cache = LLMCache(redis_client)
        self.request_limiter = RequestLimiter()
    
    @cached_llm_call(cache=None)  # 在实际使用时传入cache实例
    async def chat(self, messages: list, **kwargs) -> str:
        """带缓存的LLM调用"""
        # 限流检查
        await self.request_limiter.acquire()
        
        # 实际的LLM调用
        return await self._call_llm_api(messages, **kwargs)
```

2. **批量请求优化**
```python
import asyncio
from typing import List, Any
from dataclasses import dataclass

@dataclass
class BatchRequest:
    messages: List[dict]
    request_id: str
    priority: int = 0
    callback: callable = None

class BatchProcessor:
    def __init__(self, batch_size: int = 10, max_wait_time: float = 1.0):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
        self.processing = False
        self.stats = {"batches_processed": 0, "requests_processed": 0}
    
    async def add_request(self, request: BatchRequest) -> str:
        """添加批量请求"""
        self.pending_requests.append(request)
        
        # 如果达到批处理大小或等待时间，立即处理
        if len(self.pending_requests) >= self.batch_size:
            await self._process_batch()
        
        return request.request_id
    
    async def _process_batch(self):
        """处理批量请求"""
        if self.processing or not self.pending_requests:
            return
        
        self.processing = True
        
        try:
            # 按优先级排序
            self.pending_requests.sort(key=lambda x: x.priority, reverse=True)
            
            # 分批处理
            for i in range(0, len(self.pending_requests), self.batch_size):
                batch = self.pending_requests[i:i + self.batch_size]
                await self._execute_batch(batch)
                self.stats["batches_processed"] += 1
                self.stats["requests_processed"] += len(batch)
            
            self.pending_requests.clear()
            
        finally:
            self.processing = False
    
    async def _execute_batch(self, batch: List[BatchRequest]):
        """执行批量请求"""
        try:
            # 并行处理批量请求
            tasks = [self._process_single_request(req) for req in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # 处理结果
            for request, result in zip(batch, results):
                if isinstance(result, Exception):
                    print(f"Request {request.request_id} failed: {result}")
                    if request.callback:
                        await request.callback(None, result)
                else:
                    if request.callback:
                        await request.callback(result, None)
        
        except Exception as e:
            print(f"Batch processing error: {e}")
    
    async def _process_single_request(self, request: BatchRequest) -> Any:
        """处理单个请求"""
        # 这里应该是实际的LLM调用
        # 模拟处理
        await asyncio.sleep(0.1)
        return f"Response for {request.request_id}"
```

3. **连接池优化**
```python
import aiohttp
import asyncio
from typing import Optional
import time

class OptimizedConnectionPool:
    def __init__(self, 
                 base_url: str,
                 max_connections: int = 100,
                 connection_timeout: float = 30.0,
                 read_timeout: float = 60.0):
        self.base_url = base_url
        self.max_connections = max_connections
        self.connection_timeout = connection_timeout
        self.read_timeout = read_timeout
        
        self.session = None
        self.connection_stats = {
            "total_connections": 0,
            "active_connections": 0,
            "failed_connections": 0,
            "avg_response_time": 0.0
        }
        
        self._lock = asyncio.Lock()
    
    async def get_session(self) -> aiohttp.ClientSession:
        """获取HTTP会话"""
        if self.session is None or self.session.closed:
            async with self._lock:
                if self.session is None or self.session.closed:
                    self.session = await self._create_session()
        return self.session
    
    async def _create_session(self) -> aiohttp.ClientSession:
        """创建HTTP会话"""
        timeout = aiohttp.ClientTimeout(
            total=self.read_timeout,
            connect=self.connection_timeout
        )
        
        connector = aiohttp.TCPConnector(
            limit=self.max_connections,
            limit_per_host=self.max_connections // 4,
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                "User-Agent": "Qwen-Agent/1.0"
            }
        )
        
        return session
    
    async def request(self, method: str, url: str, **kwargs) -> dict:
        """发送HTTP请求"""
        session = await self.get_session()
        start_time = time.time()
        
        try:
            async with session.request(method, url, **kwargs) as response:
                response_time = time.time() - start_time
                
                # 更新统计信息
                self._update_stats(response_time, True)
                
                if response.status == 200:
                    return await response.json()
                else:
                    error_msg = f"HTTP {response.status}: {await response.text()}"
                    raise Exception(error_msg)
        
        except Exception as e:
            self._update_stats(time.time() - start_time, False)
            raise e
    
    def _update_stats(self, response_time: float, success: bool):
        """更新统计信息"""
        self.connection_stats["total_connections"] += 1
        
        if success:
            self.connection_stats["active_connections"] += 1
            
            # 计算平均响应时间
            total_time = (self.connection_stats["avg_response_time"] * 
                         (self.connection_stats["active_connections"] - 1) + 
                         response_time)
            self.connection_stats["avg_response_time"] = (
                total_time / self.connection_stats["active_connections"]
            )
        else:
            self.connection_stats["failed_connections"] += 1
    
    async def close(self):
        """关闭连接池"""
        if self.session and not self.session.closed:
            await self.session.close()
```

### 1.2 内存优化

**问题分析：**
- 大型对话历史占用大量内存
- 向量检索内存消耗高
- 多模态内容处理内存压力大
- 长时间运行可能出现内存泄漏

**优化方案：**

1. **对话历史压缩**
```python
import json
from typing import List, Dict, Any
from datetime import datetime, timedelta

class ConversationHistory:
    def __init__(self, max_messages: int = 100, max_age_hours: int = 24):
        self.max_messages = max_messages
        self.max_age = timedelta(hours=max_age_hours)
        self.messages = []
        self.compression_threshold = 0.7  # 压缩阈值
    
    def add_message(self, role: str, content: str, metadata: Dict = None):
        """添加消息"""
        message = {
            "role": role,
            "content": content,
            "timestamp": datetime.now(),
            "metadata": metadata or {}
        }
        
        self.messages.append(message)
        
        # 自动清理和压缩
        self._auto_cleanup()
        self._auto_compress()
    
    def _auto_cleanup(self):
        """自动清理过期消息"""
        current_time = datetime.now()
        cutoff_time = current_time - self.max_age
        
        # 移除过期消息
        self.messages = [
            msg for msg in self.messages 
            if msg["timestamp"] > cutoff_time
        ]
        
        # 如果仍然超过最大数量，移除最旧的消息
        if len(self.messages) > self.max_messages:
            self.messages = self.messages[-self.max_messages:]
    
    def _auto_compress(self):
        """自动压缩消息历史"""
        if len(self.messages) < 20:  # 太少不需要压缩
            return
        
        # 计算当前大小
        current_size = sum(len(json.dumps(msg)) for msg in self.messages)
        
        # 如果超过阈值，进行压缩
        if current_size > self.compression_threshold * 1024 * 1024:  # 1MB阈值
            self._compress_history()
    
    def _compress_history(self):
        """压缩消息历史"""
        if len(self.messages) < 10:
            return
        
        # 保留最近的10条消息
        recent_messages = self.messages[-10:]
        
        # 压缩更早的消息
        old_messages = self.messages[:-10]
        compressed_summary = self._generate_summary(old_messages)
        
        # 用摘要替换旧消息
        summary_message = {
            "role": "system",
            "content": f"Earlier conversation summary: {compressed_summary}",
            "timestamp": datetime.now(),
            "metadata": {"compressed": True, "original_count": len(old_messages)}
        }
        
        self.messages = [summary_message] + recent_messages
    
    def _generate_summary(self, messages: List[Dict]) -> str:
        """生成对话摘要"""
        # 简单的摘要算法
        user_messages = [msg for msg in messages if msg["role"] == "user"]
        assistant_messages = [msg for msg in messages if msg["role"] == "assistant"]
        
        summary_parts = []
        
        if user_messages:
            summary_parts.append(f"User asked {len(user_messages)} questions")
        
        if assistant_messages:
            summary_parts.append(f"Assistant provided {len(assistant_messages)} responses")
        
        # 提取关键主题
        all_content = " ".join([msg["content"] for msg in messages])
        key_topics = self._extract_key_topics(all_content)
        
        if key_topics:
            summary_parts.append(f"Topics discussed: {', '.join(key_topics)}")
        
        return ". ".join(summary_parts)
    
    def _extract_key_topics(self, content: str) -> List[str]:
        """提取关键主题"""
        # 简单的关键词提取
        keywords = ["AI", "model", "training", "data", "algorithm", "system", "design"]
        
        found_topics = []
        for keyword in keywords:
            if keyword.lower() in content.lower():
                found_topics.append(keyword)
        
        return found_topics[:5]  # 最多5个主题
    
    def get_recent_messages(self, count: int = 10) -> List[Dict]:
        """获取最近的消息"""
        return self.messages[-count:]
    
    def get_context_for_llm(self, max_tokens: int = 4000) -> List[Dict]:
        """获取适合LLM的上下文"""
        context = []
        current_tokens = 0
        
        # 从最新的消息开始，向前添加
        for message in reversed(self.messages):
            message_tokens = self._estimate_tokens(message["content"])
            
            if current_tokens + message_tokens <= max_tokens:
                context.insert(0, message)
                current_tokens += message_tokens
            else:
                break
        
        return context
    
    def _estimate_tokens(self, text: str) -> int:
        """估算token数量"""
        # 简单的token估算（英文按4字符=1token，中文按1字符=1token）
        if any(ord(char) > 127 for char in text):  # 包含中文
            return len(text)
        else:
            return len(text) // 4
```

2. **向量存储优化**
```python
import numpy as np
from typing import List, Dict, Any, Optional
import pickle
import os

class OptimizedVectorStore:
    def __init__(self, dimension: int = 1536, storage_path: str = "./vector_store"):
        self.dimension = dimension
        self.storage_path = storage_path
        self.vectors = []
        self.metadata = []
        self.index = None
        
        # 内存映射设置
        self.max_memory_vectors = 10000  # 内存中最多保存的向量数量
        self.disk_cache_enabled = True
        
        # 确保存储目录存在
        os.makedirs(storage_path, exist_ok=True)
    
    def add_vector(self, vector: np.ndarray, metadata: Dict[str, Any]) -> str:
        """添加向量"""
        if len(vector) != self.dimension:
            raise ValueError(f"Vector dimension mismatch. Expected {self.dimension}, got {len(vector)}")
        
        vector_id = f"vec_{len(self.vectors)}_{hash(str(vector))}"
        
        # 添加到内存
        self.vectors.append(vector.copy())
        self.metadata.append(metadata)
        
        # 如果超过内存限制，写入磁盘
        if len(self.vectors) > self.max_memory_vectors:
            self._flush_to_disk()
        
        # 重建索引
        self._rebuild_index()
        
        return vector_id
    
    def search(self, query_vector: np.ndarray, top_k: int = 5) -> List[Dict]:
        """搜索相似向量"""
        if len(self.vectors) == 0:
            return []
        
        # 如果有索引，使用索引搜索
        if self.index is not None:
            return self._index_search(query_vector, top_k)
        else:
            return self._linear_search(query_vector, top_k)
    
    def _linear_search(self, query_vector: np.ndarray, top_k: int) -> List[Dict]:
        """线性搜索（适用于少量向量）"""
        similarities = []
        
        for i, vector in enumerate(self.vectors):
            similarity = self._cosine_similarity(query_vector, vector)
            similarities.append((i, similarity))
        
        # 按相似度排序
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # 返回top_k结果
        results = []
        for i, similarity in similarities[:top_k]:
            results.append({
                "id": f"vec_{i}",
                "similarity": similarity,
                "metadata": self.metadata[i]
            })
        
        return results
    
    def _index_search(self, query_vector: np.ndarray, top_k: int) -> List[Dict]:
        """索引搜索（适用于大量向量）"""
        # 这里应该使用更高效的索引结构
        # 为了示例，我们仍然使用线性搜索
        return self._linear_search(query_vector, top_k)
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """计算余弦相似度"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _rebuild_index(self):
        """重建索引"""
        # 在实际应用中，这里应该构建FAISS或其他高效索引
        # 为了示例，我们使用简单的内存索引
        if len(self.vectors) > 1000:  # 超过1000个向量时构建索引
            self.index = "simple_index"
        else:
            self.index = None
    
    def _flush_to_disk(self):
        """将向量写入磁盘"""
        if not self.disk_cache_enabled:
            return
        
        try:
            # 保存向量和元数据
            vectors_file = os.path.join(self.storage_path, "vectors.pkl")
            metadata_file = os.path.join(self.storage_path, "metadata.pkl")
            
            with open(vectors_file, "wb") as f:
                pickle.dump(self.vectors, f)
            
            with open(metadata_file, "wb") as f:
                pickle.dump(self.metadata, f)
            
            # 清空内存中的向量（保留最近的1000个）
            if len(self.vectors) > 1000:
                self.vectors = self.vectors[-1000:]
                self.metadata = self.metadata[-1000:]
            
        except Exception as e:
            print(f"Error flushing vectors to disk: {e}")
    
    def load_from_disk(self):
        """从磁盘加载向量"""
        if not self.disk_cache_enabled:
            return
        
        try:
            vectors_file = os.path.join(self.storage_path, "vectors.pkl")
            metadata_file = os.path.join(self.storage_path, "metadata.pkl")
            
            if os.path.exists(vectors_file) and os.path.exists(metadata_file):
                with open(vectors_file, "rb") as f:
                    loaded_vectors = pickle.load(f)
                
                with open(metadata_file, "rb") as f:
                    loaded_metadata = pickle.load(f)
                
                self.vectors.extend(loaded_vectors)
                self.metadata.extend(loaded_metadata)
                
                # 重建索引
                self._rebuild_index()
                
                print(f"Loaded {len(loaded_vectors)} vectors from disk")
        
        except Exception as e:
            print(f"Error loading vectors from disk: {e}")
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """获取内存使用情况"""
        import sys
        
        vectors_size = sys.getsizeof(self.vectors)
        metadata_size = sys.getsizeof(self.metadata)
        
        return {
            "vector_count": len(self.vectors),
            "vectors_memory_mb": vectors_size / (1024 * 1024),
            "metadata_memory_mb": metadata_size / (1024 * 1024),
            "total_memory_mb": (vectors_size + metadata_size) / (1024 * 1024),
            "index_built": self.index is not None
        }
```

### 1.3 并发优化

**问题分析：**
- 多用户并发访问时性能下降
- 工具调用串行执行效率低
- 资源竞争导致延迟增加
- 线程池管理不当

**优化方案：**

1. **智能并发控制**
```python
import asyncio
import threading
from typing import Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from dataclasses import dataclass

@dataclass
class ConcurrencyConfig:
    max_concurrent_requests: int = 100
    max_concurrent_tools: int = 10
    max_concurrent_llm_calls: int = 50
    request_timeout: float = 30.0
    tool_timeout: float = 60.0

class SmartConcurrencyManager:
    def __init__(self, config: ConcurrencyConfig):
        self.config = config
        self.active_requests = 0
        self.active_tools = 0
        self.active_llm_calls = 0
        
        # 信号量控制
        self.request_semaphore = asyncio.Semaphore(config.max_concurrent_requests)
        self.tool_semaphore = asyncio.Semaphore(config.max_concurrent_tools)
        self.llm_semaphore = asyncio.Semaphore(config.max_concurrent_llm_calls)
        
        # 统计信息
        self.stats = {
            "total_requests": 0,
            "completed_requests": 0,
            "failed_requests": 0,
            "avg_response_time": 0.0,
            "peak_concurrent_requests": 0
        }
        
        # 自适应调整
        self.performance_history = []
        self.adjustment_interval = 60  # 60秒调整一次
        self.last_adjustment = time.time()
    
    async def acquire_request_slot(self) -> bool:
        """获取请求槽位"""
        try:
            await asyncio.wait_for(
                self.request_semaphore.acquire(),
                timeout=self.config.request_timeout
            )
            
            self.active_requests += 1
            self.stats["total_requests"] += 1
            self.stats["peak_concurrent_requests"] = max(
                self.stats["peak_concurrent_requests"],
                self.active_requests
            )
            
            return True
            
        except asyncio.TimeoutError:
            return False
    
    async def release_request_slot(self, response_time: float, success: bool):
        """释放请求槽位"""
        self.request_semaphore.release()
        self.active_requests -= 1
        
        # 更新统计
        self.stats["completed_requests"] += 1
        if not success:
            self.stats["failed_requests"] += 1
        
        # 更新平均响应时间
        total_time = (self.stats["avg_response_time"] * 
                     (self.stats["completed_requests"] - 1) + 
                     response_time)
        self.stats["avg_response_time"] = total_time / self.stats["completed_requests"]
        
        # 记录性能历史
        self.performance_history.append({
            "timestamp": time.time(),
            "response_time": response_time,
            "success": success,
            "concurrent_requests": self.active_requests
        })
        
        # 自适应调整
        self._auto_adjust()
    
    async def execute_tool_call(self, tool_func, *args, **kwargs):
        """执行工具调用"""
        async with self.tool_semaphore:
            try:
                result = await asyncio.wait_for(
                    tool_func(*args, **kwargs),
                    timeout=self.config.tool_timeout
                )
                return result, True
            except asyncio.TimeoutError:
                return None, False
            except Exception as e:
                return str(e), False
    
    async def execute_llm_call(self, llm_func, *args, **kwargs):
        """执行LLM调用"""
        async with self.llm_semaphore:
            try:
                result = await asyncio.wait_for(
                    llm_func(*args, **kwargs),
                    timeout=self.config.request_timeout
                )
                return result, True
            except asyncio.TimeoutError:
                return None, False
            except Exception as e:
                return str(e), False
    
    def _auto_adjust(self):
        """自适应调整并发参数"""
        current_time = time.time()
        
        if current_time - self.last_adjustment < self.adjustment_interval:
            return
        
        self.last_adjustment = current_time
        
        # 分析最近性能
        recent_performance = self._get_recent_performance()
        
        if len(recent_performance) < 10:
            return
        
        # 计算关键指标
        avg_response_time = np.mean([p["response_time"] for p in recent_performance])
        success_rate = np.mean([p["success"] for p in recent_performance])
        avg_concurrent = np.mean([p["concurrent_requests"] for p in recent_performance])
        
        # 调整策略
        if success_rate < 0.9 or avg_response_time > 10.0:
            # 性能不佳，减少并发
            self.config.max_concurrent_requests = max(
                10, int(self.config.max_concurrent_requests * 0.8)
            )
            print(f"Reduced max concurrent requests to {self.config.max_concurrent_requests}")
        
        elif success_rate > 0.95 and avg_response_time < 2.0:
            # 性能良好，可以增加并发
            self.config.max_concurrent_requests = min(
                200, int(self.config.max_concurrent_requests * 1.2)
            )
            print(f"Increased max concurrent requests to {self.config.max_concurrent_requests}")
    
    def _get_recent_performance(self, duration: int = 300) -> List[Dict]:
        """获取最近的性能数据"""
        cutoff_time = time.time() - duration
        return [
            p for p in self.performance_history
            if p["timestamp"] > cutoff_time
        ]
    
    def get_status(self) -> Dict[str, Any]:
        """获取状态信息"""
        return {
            "active_requests": self.active_requests,
            "active_tools": self.active_tools,
            "active_llm_calls": self.active_llm_calls,
            "config": self.config.__dict__,
            "stats": self.stats,
            "queue_lengths": {
                "requests": self.config.max_concurrent_requests - self.active_requests,
                "tools": self.config.max_concurrent_tools - self.active_tools,
                "llm": self.config.max_concurrent_llm_calls - self.active_llm_calls
            }
        }
```

## 2. 架构优化

### 2.1 微服务化架构

**问题分析：**
- 单体架构扩展性有限
- 组件间耦合度高
- 难以独立优化和部署
- 故障隔离性差

**优化方案：**

1. **服务拆分设计**
```python
# 服务接口定义
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
import asyncio

class AgentService(ABC):
    @abstractmethod
    async def process_message(self, request: Dict[str, Any]) -> Dict[str, Any]:
        pass
    
    @abstractmethod
    async def get_agent_info(self, agent_id: str) -> Dict[str, Any]:
        pass

class LLMService(ABC):
    @abstractmethod
    async def chat(self, request: Dict[str, Any]) -> Dict[str, Any]:
        pass
    
    @abstractmethod
    async def get_model_info(self, model_id: str) -> Dict[str, Any]:
        pass

class ToolService(ABC):
    @abstractmethod
    async def execute_tool(self, request: Dict[str, Any]) -> Dict[str, Any]:
        pass
    
    @abstractmethod
    async def get_tool_info(self, tool_id: str) -> Dict[str, Any]:
        pass

# 服务实现示例
class AgentServiceImpl(AgentService):
    def __init__(self, llm_service: LLMService, tool_service: ToolService):
        self.llm_service = llm_service
        self.tool_service = tool_service
        self.agents: Dict[str, Dict] = {}
        self.concurrency_manager = SmartConcurrencyManager(ConcurrencyConfig())
    
    async def process_message(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """处理消息"""
        agent_id = request.get("agent_id")
        message = request.get("message")
        conversation_id = request.get("conversation_id")
        
        # 获取请求槽位
        if not await self.concurrency_manager.acquire_request_slot():
            return {"error": "System busy, please try again later"}
        
        start_time = time.time()
        
        try:
            # 获取Agent配置
            agent_config = await self.get_agent_info(agent_id)
            if not agent_config:
                return {"error": f"Agent {agent_id} not found"}
            
            # 处理消息
            response = await self._process_with_agent(
                agent_config, message, conversation_id
            )
            
            response_time = time.time() - start_time
            await self.concurrency_manager.release_request_slot(response_time, True)
            
            return response
            
        except Exception as e:
            response_time = time.time() - start_time
            await self.concurrency_manager.release_request_slot(response_time, False)
            
            return {"error": str(e)}
    
    async def _process_with_agent(self, 
                                agent_config: Dict, 
                                message: str, 
                                conversation_id: str) -> Dict[str, Any]:
        """使用Agent处理消息"""
        # 检查是否需要工具调用
        if agent_config.get("has_tools", False):
            # 先调用LLM生成响应
            llm_request = {
                "messages": [
                    {"role": "user", "content": message}
                ],
                "system_message": agent_config.get("system_message"),
                "model": agent_config.get("model")
            }
            
            llm_response = await self.llm_service.chat(llm_request)
            
            # 解析工具调用
            tool_calls = self._parse_tool_calls(llm_response.get("content", ""))
            
            if tool_calls:
                # 执行工具调用
                tool_results = await self._execute_tools(tool_calls)
                
                # 生成最终响应
                final_request = {
                    "messages": [
                        {"role": "user", "content": message},
                        {"role": "assistant", "content": llm_response.get("content")},
                        {"role": "system", "content": f"Tool results: {tool_results}"}
                    ],
                    "model": agent_config.get("model")
                }
                
                final_response = await self.llm_service.chat(final_request)
                return final_response
            else:
                return llm_response
        else:
            # 直接LLM调用
            llm_request = {
                "messages": [{"role": "user", "content": message}],
                "system_message": agent_config.get("system_message"),
                "model": agent_config.get("model")
            }
            
            return await self.llm_service.chat(llm_request)
    
    async def _execute_tools(self, tool_calls: List[Dict]) -> List[Dict]:
        """执行工具调用"""
        results = []
        
        # 并行执行工具调用
        tasks = []
        for call in tool_calls:
            task = self.tool_service.execute_tool({
                "tool_name": call["name"],
                "parameters": call.get("parameters", {}),
                "timeout": 30.0
            })
            tasks.append(task)
        
        try:
            tool_responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            for i, response in enumerate(tool_responses):
                if isinstance(response, Exception):
                    results.append({
                        "tool_name": tool_calls[i]["name"],
                        "error": str(response)
                    })
                else:
                    results.append(response)
        
        except Exception as e:
            results.append({"error": str(e)})
        
        return results
    
    def _parse_tool_calls(self, content: str) -> List[Dict]:
        """解析工具调用"""
        # 简化的工具调用解析
        import re
        
        tool_calls = []
        
        # 查找函数调用模式
        pattern = r'(\w+)\s*\(([^)]*)\)'
        matches = re.findall(pattern, content)
        
        for func_name, args_str in matches:
            # 解析参数
            args = {}
            if args_str.strip():
                for arg_pair in args_str.split(','):
                    if '=' in arg_pair:
                        key, value = arg_pair.split('=', 1)
                        args[key.strip()] = value.strip().strip('"\'')
            
            tool_calls.append({
                "name": func_name,
                "parameters": args
            })
        
        return tool_calls
```

2. **服务发现和负载均衡**
```python
import consul
import random
import time
from typing import List, Dict, Any, Optional

class ServiceRegistry:
    def __init__(self, consul_host: str = "localhost", consul_port: int = 8500):
        self.consul = consul.Consul(host=consul_host, port=consul_port)
        self.service_cache = {}
        self.cache_ttl = 30  # 缓存30秒
        self.last_cache_update = {}
    
    async def register_service(self, 
                             service_name: str, 
                             service_id: str,
                             address: str, 
                             port: int,
                             tags: List[str] = None):
        """注册服务"""
        try:
            self.consul.agent.service.register(
                name=service_name,
                service_id=service_id,
                address=address,
                port=port,
                tags=tags or [],
                check=consul.Check.http(
                    f"http://{address}:{port}/health",
                    interval="10s",
                    timeout="5s"
                )
            )
            print(f"Registered service {service_name} with ID {service_id}")
            
        except Exception as e:
            print(f"Failed to register service {service_name}: {e}")
    
    async def discover_service(self, service_name: str) -> List[Dict[str, Any]]:
        """发现服务实例"""
        # 检查缓存
        if self._is_cache_valid(service_name):
            return self.service_cache[service_name]
        
        try:
            # 从Consul获取服务
            _, services = self.consul.health.service(service_name, passing=True)
            
            instances = []
            for service in services:
                instances.append({
                    "id": service["Service"]["ID"],
                    "name": service["Service"]["Service"],
                    "address": service["Service"]["Address"],
                    "port": service["Service"]["Port"],
                    "tags": service["Service"]["Tags"]
                })
            
            # 更新缓存
            self.service_cache[service_name] = instances
            self.last_cache_update[service_name] = time.time()
            
            return instances
            
        except Exception as e:
            print(f"Failed to discover service {service_name}: {e}")
            return []
    
    def _is_cache_valid(self, service_name: str) -> bool:
        """检查缓存是否有效"""
        if service_name not in self.service_cache:
            return False
        
        if service_name not in self.last_cache_update:
            return False
        
        return time.time() - self.last_cache_update[service_name] < self.cache_ttl

class LoadBalancer:
    def __init__(self, service_registry: ServiceRegistry):
        self.service_registry = service_registry
        self.strategies = {
            "round_robin": RoundRobinStrategy(),
            "least_connections": LeastConnectionsStrategy(),
            "weighted": WeightedStrategy()
        }
        self.current_strategy = "round_robin"
        self.connection_counts = {}
    
    async def select_instance(self, service_name: str) -> Optional[Dict[str, Any]]:
        """选择服务实例"""
        instances = await self.service_registry.discover_service(service_name)
        
        if not instances:
            return None
        
        strategy = self.strategies[self.current_strategy]
        return await strategy.select(instances, self.connection_counts)
    
    def set_strategy(self, strategy_name: str):
        """设置负载均衡策略"""
        if strategy_name in self.strategies:
            self.current_strategy = strategy_name
    
    def record_connection(self, instance_id: str):
        """记录连接"""
        if instance_id not in self.connection_counts:
            self.connection_counts[instance_id] = 0
        self.connection_counts[instance_id] += 1
    
    def record_disconnection(self, instance_id: str):
        """记录断开连接"""
        if instance_id in self.connection_counts:
            self.connection_counts[instance_id] = max(
                0, self.connection_counts[instance_id] - 1
            )

class RoundRobinStrategy:
    def __init__(self):
        self.counters = {}
    
    async def select(self, instances: List[Dict], connection_counts: Dict) -> Dict:
        """轮询选择"""
        service_name = instances[0]["name"]
        if service_name not in self.counters:
            self.counters[service_name] = 0
        
        index = self.counters[service_name] % len(instances)
        self.counters[service_name] += 1
        
        return instances[index]

class LeastConnectionsStrategy:
    async def select(self, instances: List[Dict], connection_counts: Dict) -> Dict:
        """最少连接选择"""
        # 获取每个实例的连接数
        instance_connections = []
        for instance in instances:
            conn_count = connection_counts.get(instance["id"], 0)
            instance_connections.append((instance, conn_count))
        
        # 选择连接数最少的实例
        instance_connections.sort(key=lambda x: x[1])
        return instance_connections[0][0]

class WeightedStrategy:
    async def select(self, instances: List[Dict], connection_counts: Dict) -> Dict:
        """加权选择"""
        # 计算权重总和
        total_weight = 0
        weighted_instances = []
        
        for instance in instances:
            # 权重可以基于实例的性能、配置等
            weight = self._calculate_weight(instance, connection_counts)
            weighted_instances.append((instance, weight))
            total_weight += weight
        
        # 随机选择
        if total_weight == 0:
            return random.choice(instances)
        
        import random
        rand_val = random.uniform(0, total_weight)
        
        current_weight = 0
        for instance, weight in weighted_instances:
            current_weight += weight
            if rand_val <= current_weight:
                return instance
        
        return weighted_instances[-1][0]
    
    def _calculate_weight(self, instance: Dict, connection_counts: Dict) -> float:
        """计算实例权重"""
        # 基础权重
        weight = 1.0
        
        # 根据连接数调整权重
        conn_count = connection_counts.get(instance["id"], 0)
        weight *= (1.0 / (1.0 + conn_count * 0.1))
        
        # 根据标签调整权重
        tags = instance.get("tags", [])
        if "high-performance" in tags:
            weight *= 1.5
        elif "low-performance" in tags:
            weight *= 0.7
        
        return weight
```

### 2.2 事件驱动架构

**问题分析：**
- 同步调用耦合度高
- 系统响应慢
- 难以处理异步事件
- 扩展性受限

**优化方案：**

1. **事件总线设计**
```python
import asyncio
import json
from typing import Dict, Any, List, Callable, Optional
from dataclasses import dataclass
from enum import Enum
import uuid
import time

class EventType(Enum):
    AGENT_MESSAGE = "agent_message"
    LLM_RESPONSE = "llm_response"
    TOOL_EXECUTION = "tool_execution"
    ERROR_OCCURRED = "error_occurred"
    SYSTEM_METRICS = "system_metrics"

@dataclass
class Event:
    event_type: EventType
    data: Dict[str, Any]
    timestamp: float
    event_id: str
    source: str
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

class EventBus:
    def __init__(self):
        self.subscribers: Dict[EventType, List[Callable]] = {}
        self.event_history: List[Event] = []
        self.max_history_size = 1000
        self.metrics = {
            "events_published": 0,
            "events_processed": 0,
            "processing_errors": 0
        }
    
    def subscribe(self, event_type: EventType, handler: Callable):
        """订阅事件"""
        if event_type not in self.subscribers:
            self.subscribers[event_type] = []
        
        self.subscribers[event_type].append(handler)
        print(f"Subscribed to {event_type.value}")
    
    def unsubscribe(self, event_type: EventType, handler: Callable):
        """取消订阅"""
        if event_type in self.subscribers:
            try:
                self.subscribers[event_type].remove(handler)
                print(f"Unsubscribed from {event_type.value}")
            except ValueError:
                pass
    
    async def publish(self, event: Event):
        """发布事件"""
        try:
            # 添加到历史记录
            self.event_history.append(event)
            if len(self.event_history) > self.max_history_size:
                self.event_history = self.event_history[-self.max_history_size:]
            
            # 更新指标
            self.metrics["events_published"] += 1
            
            # 异步处理事件
            asyncio.create_task(self._process_event(event))
            
        except Exception as e:
            print(f"Error publishing event: {e}")
            self.metrics["processing_errors"] += 1
    
    async def _process_event(self, event: Event):
        """处理事件"""
        try:
            if event.event_type in self.subscribers:
                # 并行调用所有订阅者
                tasks = []
                for handler in self.subscribers[event.event_type]:
                    task = asyncio.create_task(self._call_handler(handler, event))
                    tasks.append(task)
                
                # 等待所有处理完成
                await asyncio.gather(*tasks, return_exceptions=True)
                
                self.metrics["events_processed"] += 1
            
        except Exception as e:
            print(f"Error processing event {event.event_type.value}: {e}")
            self.metrics["processing_errors"] += 1
    
    async def _call_handler(self, handler: Callable, event: Event):
        """调用事件处理器"""
        try:
            if asyncio.iscoroutinefunction(handler):
                await handler(event)
            else:
                handler(event)
        except Exception as e:
            print(f"Error in event handler: {e}")
    
    def create_event(self, 
                   event_type: EventType, 
                   data: Dict[str, Any], 
                   source: str = "unknown",
                   metadata: Dict[str, Any] = None) -> Event:
        """创建事件"""
        return Event(
            event_type=event_type,
            data=data,
            timestamp=time.time(),
            event_id=str(uuid.uuid4()),
            source=source,
            metadata=metadata or {}
        )
    
    def get_metrics(self) -> Dict[str, Any]:
        """获取指标"""
        return self.metrics.copy()
    
    def get_recent_events(self, event_type: Optional[EventType] = None, 
                        limit: int = 50) -> List[Event]:
        """获取最近的事件"""
        events = self.event_history
        
        if event_type:
            events = [e for e in events if e.event_type == event_type]
        
        return events[-limit:]

# 事件处理器示例
class AgentEventHandler:
    def __init__(self, event_bus: EventBus):
        self.event_bus = event_bus
        self.setup_handlers()
    
    def setup_handlers(self):
        """设置事件处理器"""
        self.event_bus.subscribe(EventType.AGENT_MESSAGE, self.handle_agent_message)
        self.event_bus.subscribe(EventType.LLM_RESPONSE, self.handle_llm_response)
        self.event_bus.subscribe(EventType.ERROR_OCCURRED, self.handle_error)
    
    async def handle_agent_message(self, event: Event):
        """处理Agent消息事件"""
        data = event.data
        agent_id = data.get("agent_id")
        message = data.get("message")
        
        print(f"Agent {agent_id} sent message: {message[:100]}...")
        
        # 可以在这里添加消息处理逻辑
        # 例如：记录日志、触发其他事件等
    
    async def handle_llm_response(self, event: Event):
        """处理LLM响应事件"""
        data = event.data
        model = data.get("model")
        response_time = data.get("response_time")
        
        print(f"LLM {model} responded in {response_time:.2f}s")
        
        # 可以在这里添加响应处理逻辑
        # 例如：性能监控、质量检查等
    
    async def handle_error(self, event: Event):
        """处理错误事件"""
        data = event.data
        error_type = data.get("error_type")
        error_message = data.get("error_message")
        
        print(f"Error occurred: {error_type} - {error_message}")
        
        # 可以在这里添加错误处理逻辑
        # 例如：告警、重试等

# 事件驱动的Agent服务
class EventDrivenAgentService:
    def __init__(self, event_bus: EventBus, llm_service, tool_service):
        self.event_bus = event_bus
        self.llm_service = llm_service
        self.tool_service = tool_service
        self.active_conversations = {}
    
    async def process_message_event(self, event: Event):
        """处理消息事件"""
        data = event.data
        conversation_id = data.get("conversation_id")
        message = data.get("message")
        agent_id = data.get("agent_id")
        
        # 创建处理上下文
        context = {
            "conversation_id": conversation_id,
            "agent_id": agent_id,
            "start_time": time.time()
        }
        
        try:
            # 处理消息
            response = await self._process_message_with_context(context, message)
            
            # 发布响应事件
            response_event = self.event_bus.create_event(
                EventType.LLM_RESPONSE,
                {
                    "conversation_id": conversation_id,
                    "response": response,
                    "response_time": time.time() - context["start_time"]
                },
                source="agent_service"
            )
            
            await self.event_bus.publish(response_event)
            
        except Exception as e:
            # 发布错误事件
            error_event = self.event_bus.create_event(
                EventType.ERROR_OCCURRED,
                {
                    "conversation_id": conversation_id,
                    "error_type": "processing_error",
                    "error_message": str(e)
                },
                source="agent_service"
            )
            
            await self.event_bus.publish(error_event)
    
    async def _process_message_with_context(self, context: Dict, message: str) -> str:
        """在上下文中处理消息"""
        # 这里应该是实际的Agent处理逻辑
        # 为了示例，我们模拟一个简单的处理
        
        # 检查是否需要工具
        if "calculate" in message.lower():
            # 发布工具执行事件
            tool_event = self.event_bus.create_event(
                EventType.TOOL_EXECUTION,
                {
                    "tool_name": "calculator",
                    "parameters": {"expression": message},
                    "conversation_id": context["conversation_id"]
                },
                source="agent_service"
            )
            
            await self.event_bus.publish(tool_event)
            
            # 模拟工具执行
            await asyncio.sleep(0.5)
            
            return f"Calculation result for: {message}"
        else:
            # 直接LLM调用
            llm_request = {
                "messages": [{"role": "user", "content": message}],
                "model": "qwen-turbo"
            }
            
            llm_response = await self.llm_service.chat(llm_request)
            return llm_response.get("content", "No response")
```

## 3. 功能优化

### 3.1 工具系统优化

**问题分析：**
- 工具调用效率低
- 错误处理不完善
- 工具发现机制不够智能
- 安全性考虑不足

**优化方案：**

1. **智能工具选择**
```python
import numpy as np
from typing import List, Dict, Any, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import os

class ToolSelectionEngine:
    def __init__(self, tool_descriptions: Dict[str, str]):
        self.tool_descriptions = tool_descriptions
        self.vectorizer = TfidfVectorizer(stop_words='english')
        self.tool_vectors = None
        self.usage_stats = {}
        self.performance_metrics = {}
        
        # 构建工具向量
        self._build_tool_vectors()
    
    def _build_tool_vectors(self):
        """构建工具描述向量"""
        descriptions = list(self.tool_descriptions.values())
        self.tool_vectors = self.vectorizer.fit_transform(descriptions)
    
    def select_tools(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """选择相关工具"""
        if not self.tool_descriptions:
            return []
        
        # 向量化查询
        query_vector = self.vectorizer.transform([query])
        
        # 计算相似度
        similarities = cosine_similarity(query_vector, self.tool_vectors)[0]
        
        # 结合使用统计和性能指标
        enhanced_scores = self._enhance_scores(similarities)
        
        # 排序并返回top_k
        tool_names = list(self.tool_descriptions.keys())
        scored_tools = list(zip(tool_names, enhanced_scores))
        scored_tools.sort(key=lambda x: x[1], reverse=True)
        
        return scored_tools[:top_k]
    
    def _enhance_scores(self, similarities: np.ndarray) -> np.ndarray:
        """增强相似度分数"""
        enhanced = similarities.copy()
        
        for i, tool_name in enumerate(self.tool_descriptions.keys()):
            # 使用统计权重
            usage_weight = self._get_usage_weight(tool_name)
            
            # 性能权重
            performance_weight = self._get_performance_weight(tool_name)
            
            # 最新度权重
            recency_weight = self._get_recency_weight(tool_name)
            
            # 综合权重
            total_weight = usage_weight * 0.3 + performance_weight * 0.4 + recency_weight * 0.3
            enhanced[i] *= total_weight
        
        return enhanced
    
    def _get_usage_weight(self, tool_name: str) -> float:
        """获取使用权重"""
        stats = self.usage_stats.get(tool_name, {"count": 0, "last_used": 0})
        
        if stats["count"] == 0:
            return 1.0  # 新工具给予正常权重
        
        # 使用频率权重
        usage_freq = min(stats["count"] / 100.0, 2.0)  # 最多2倍权重
        
        return usage_freq
    
    def _get_performance_weight(self, tool_name: str) -> float:
        """获取性能权重"""
        metrics = self.performance_metrics.get(tool_name, {"success_rate": 1.0, "avg_time": 1.0})
        
        success_rate = metrics["success_rate"]
        avg_time = metrics["avg_time"]
        
        # 成功率权重
        success_weight = success_rate
        
        # 时间权重（时间越短权重越高）
        time_weight = 1.0 / max(avg_time, 0.1)
        
        return (success_weight + time_weight) / 2.0
    
    def _get_recency_weight(self, tool_name: str) -> float:
        """获取最新度权重"""
        stats = self.usage_stats.get(tool_name, {"last_used": 0})
        
        if stats["last_used"] == 0:
            return 0.8  # 未使用过的工具给予较低权重
        
        # 时间衰减
        time_diff = time.time() - stats["last_used"]
        recency_weight = np.exp(-time_diff / (7 * 24 * 3600))  # 7天衰减
        
        return max(0.5, recency_weight)
    
    def record_tool_usage(self, tool_name: str, success: bool, execution_time: float):
        """记录工具使用情况"""
        # 更新使用统计
        if tool_name not in self.usage_stats:
            self.usage_stats[tool_name] = {"count": 0, "last_used": 0}
        
        self.usage_stats[tool_name]["count"] += 1
        self.usage_stats[tool_name]["last_used"] = time.time()
        
        # 更新性能指标
        if tool_name not in self.performance_metrics:
            self.performance_metrics[tool_name] = {
                "success_count": 0,
                "total_count": 0,
                "total_time": 0.0
            }
        
        metrics = self.performance_metrics[tool_name]
        metrics["total_count"] += 1
        metrics["total_time"] += execution_time
        
        if success:
            metrics["success_count"] += 1
        
        # 计算派生指标
        metrics["success_rate"] = metrics["success_count"] / metrics["total_count"]
        metrics["avg_time"] = metrics["total_time"] / metrics["total_count"]
    
    def save_model(self, path: str):
        """保存模型"""
        model_data = {
            "vectorizer": self.vectorizer,
            "tool_vectors": self.tool_vectors,
            "tool_descriptions": self.tool_descriptions,
            "usage_stats": self.usage_stats,
            "performance_metrics": self.performance_metrics
        }
        
        with open(path, 'wb') as f:
            pickle.dump(model_data, f)
    
    def load_model(self, path: str):
        """加载模型"""
        if os.path.exists(path):
            with open(path, 'rb') as f:
                model_data = pickle.load(f)
            
            self.vectorizer = model_data["vectorizer"]
            self.tool_vectors = model_data["tool_vectors"]
            self.tool_descriptions = model_data["tool_descriptions"]
            self.usage_stats = model_data.get("usage_stats", {})
            self.performance_metrics = model_data.get("performance_metrics", {})

# 工具执行优化器
class ToolExecutionOptimizer:
    def __init__(self, max_concurrent: int = 5, timeout: float = 30.0):
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.execution_history = []
        self.performance_profiles = {}
    
    async def execute_tools(self, tool_calls: List[Dict]) -> List[Dict]:
        """优化执行工具调用"""
        # 分析工具依赖关系
        dependency_graph = self._analyze_dependencies(tool_calls)
        
        # 分组执行（无依赖的可以并行）
        execution_groups = self._group_by_dependencies(dependency_graph)
        
        results = []
        
        for group in execution_groups:
            # 并行执行组内工具
            group_results = await self._execute_group(group)
            results.extend(group_results)
        
        return results
    
    def _analyze_dependencies(self, tool_calls: List[Dict]) -> Dict[str, List[str]]:
        """分析工具依赖关系"""
        dependencies = {}
        
        for call in tool_calls:
            tool_name = call["name"]
            dependencies[tool_name] = []
            
            # 分析参数中的依赖
            parameters = call.get("parameters", {})
            for param_value in parameters.values():
                if isinstance(param_value, str) and "{{" in param_value:
                    # 提取依赖的工具名称
                    dependencies[tool_name].extend(
                        self._extract_tool_dependencies(param_value)
                    )
        
        return dependencies
    
    def _extract_tool_dependencies(self, text: str) -> List[str]:
        """从文本中提取工具依赖"""
        import re
        
        # 查找 {{tool_name}} 模式
        pattern = r'\{\{(\w+)\}\}'
        matches = re.findall(pattern, text)
        
        return matches
    
    def _group_by_dependencies(self, dependencies: Dict[str, List[str]]) -> List[List[str]]:
        """按依赖关系分组"""
        groups = []
        processed = set()
        
        while len(processed) < len(dependencies):
            # 找出没有未处理依赖的工具
            current_group = []
            
            for tool_name in dependencies:
                if tool_name not in processed:
                    deps = dependencies[tool_name]
                    # 所有依赖都已处理
                    if all(dep in processed for dep in deps):
                        current_group.append(tool_name)
            
            if not current_group:
                # 如果没有符合条件的工具，可能存在循环依赖
                remaining = [t for t in dependencies if t not in processed]
                current_group = remaining[:1]  # 一次处理一个
            
            groups.append(current_group)
            processed.update(current_group)
        
        return groups
    
    async def _execute_group(self, tool_names: List[str]) -> List[Dict]:
        """执行一组工具"""
        if not tool_names:
            return []
        
        # 限制并发数量
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def execute_with_semaphore(tool_name: str):
            async with semaphore:
                return await self._execute_single_tool(tool_name)
        
        # 并行执行
        tasks = [execute_with_semaphore(name) for name in tool_names]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results
    
    async def _execute_single_tool(self, tool_name: str) -> Dict:
        """执行单个工具"""
        start_time = time.time()
        
        try:
            # 获取工具实例
            tool_instance = self._get_tool_instance(tool_name)
            
            # 执行工具
            result = await asyncio.wait_for(
                tool_instance.call(),
                timeout=self.timeout
            )
            
            execution_time = time.time() - start_time
            
            # 记录执行历史
            self._record_execution(tool_name, execution_time, True)
            
            return {
                "tool_name": tool_name,
                "result": result,
                "success": True,
                "execution_time": execution_time
            }
            
        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            self._record_execution(tool_name, execution_time, False)
            
            return {
                "tool_name": tool_name,
                "error": "Tool execution timeout",
                "success": False,
                "execution_time": execution_time
            }
        
        except Exception as e:
            execution_time = time.time() - start_time
            self._record_execution(tool_name, execution_time, False)
            
            return {
                "tool_name": tool_name,
                "error": str(e),
                "success": False,
                "execution_time": execution_time
            }
    
    def _get_tool_instance(self, tool_name: str):
        """获取工具实例"""
        # 这里应该从工具注册中心获取
        # 为了示例，我们返回一个模拟的工具实例
        class MockTool:
            async def call(self):
                await asyncio.sleep(0.5)
                return f"Mock result for {tool_name}"
        
        return MockTool()
    
    def _record_execution(self, tool_name: str, execution_time: float, success: bool):
        """记录执行历史"""
        self.execution_history.append({
            "tool_name": tool_name,
            "execution_time": execution_time,
            "success": success,
            "timestamp": time.time()
        })
        
        # 更新性能档案
        if tool_name not in self.performance_profiles:
            self.performance_profiles[tool_name] = {
                "avg_time": execution_time,
                "success_rate": 1.0 if success else 0.0,
                "execution_count": 1
            }
        else:
            profile = self.performance_profiles[tool_name]
            
            # 更新平均时间
            profile["avg_time"] = (
                (profile["avg_time"] * profile["execution_count"] + execution_time) /
                (profile["execution_count"] + 1)
            )
            
            # 更新成功率
            if success:
                profile["success_rate"] = (
                    (profile["success_rate"] * profile["execution_count"] + 1) /
                    (profile["execution_count"] + 1)
                )
            else:
                profile["success_rate"] = (
                    profile["success_rate"] * profile["execution_count"] /
                    (profile["execution_count"] + 1)
                )
            
            profile["execution_count"] += 1
```

### 3.2 对话管理优化

**问题分析：**
- 长对话上下文管理困难
- 对话历史存储成本高
- 上下文相关性分析不够智能
- 多轮对话连贯性不足

**优化方案：**

1. **智能上下文管理**
```python
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import json

class ConversationContextManager:
    def __init__(self, max_context_length: int = 10000, 
                 relevance_threshold: float = 0.3):
        self.max_context_length = max_context_length
        self.relevance_threshold = relevance_threshold
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.context_vectors = None
        self.conversation_history = []
        
    def add_message(self, role: str, content: str, metadata: Dict = None):
        """添加消息到对话历史"""
        message = {
            "role": role,
            "content": content,
            "timestamp": time.time(),
            "metadata": metadata or {}
        }
        
        self.conversation_history.append(message)
        
        # 更新上下文向量
        self._update_context_vectors()
    
    def _update_context_vectors(self):
        """更新上下文向量"""
        if len(self.conversation_history) < 2:
            return
        
        # 构建文本用于向量化
        texts = []
        for msg in self.conversation_history:
            text = f"{msg['role']}: {msg['content']}"
            texts.append(text)
        
        # 更新向量化器
        if self.context_vectors is None:
            self.context_vectors = self.vectorizer.fit_transform(texts)
        else:
            new_vectors = self.vectorizer.transform(texts[-1:])
            self.context_vectors = np.vstack([self.context_vectors, new_vectors])
    
    def get_relevant_context(self, current_query: str, max_messages: int = 10) -> List[Dict]:
        """获取相关的上下文"""
        if not self.conversation_history:
            return []
        
        # 计算查询与历史消息的相似度
        query_vector = self.vectorizer.transform([current_query])
        
        if self.context_vectors is None:
            # 如果没有向量，返回最近的消息
            return self.conversation_history[-max_messages:]
        
        similarities = cosine_similarity(query_vector, self.context_vectors)[0]
        
        # 选择相关性高的消息
        relevant_indices = np.where(similarities > self.relevance_threshold)[0]
        
        if len(relevant_indices) == 0:
            # 如果没有高度相关的，选择相似度最高的几个
            top_indices = np.argsort(similarities)[-max_messages:]
            relevant_indices = top_indices
        
        # 按时间顺序排序
        relevant_indices = sorted(relevant_indices)
        
        # 获取对应的消息
        relevant_messages = [self.conversation_history[i] for i in relevant_indices]
        
        # 限制消息数量
        return relevant_messages[-max_messages:]
    
    def get_condensed_context(self, max_tokens: int = 4000) -> List[Dict]:
        """获取压缩的上下文"""
        if not self.conversation_history:
            return []
        
        # 按重要性排序消息
        important_messages = self._rank_messages_by_importance()
        
        # 选择最重要的消息，直到达到token限制
        condensed_context = []
        current_tokens = 0
        
        for message in important_messages:
            message_tokens = self._estimate_tokens(message["content"])
            
            if current_tokens + message_tokens <= max_tokens:
                condensed_context.append(message)
                current_tokens += message_tokens
            else:
                break
        
        return condensed_context
    
    def _rank_messages_by_importance(self) -> List[Dict]:
        """按重要性对消息排序"""
        if len(self.conversation_history) <= 3:
            return self.conversation_history.copy()
        
        # 计算每个消息的重要性分数
        importance_scores = []
        
        for i, message in enumerate(self.conversation_history):
            score = 0.0
            
            # 1. 时间权重（最近的更重要）
            time_weight = (i + 1) / len(self.conversation_history)
            score += time_weight * 0.3
            
            # 2. 内容长度权重（长的可能更重要）
            content_length = len(message["content"])
            length_weight = min(content_length / 500.0, 1.0)
            score += length_weight * 0.2
            
            # 3. 角色权重（用户和助手消息更重要）
            if message["role"] in ["user", "assistant"]:
                score += 0.3
            
            # 4. 关键词权重
            keyword_score = self._calculate_keyword_score(message["content"])
            score += keyword_score * 0.2
            
            importance_scores.append((message, score))
        
        # 按重要性排序
        importance_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [msg for msg, score in importance_scores]
    
    def _calculate_keyword_score(self, content: str) -> float:
        """计算关键词分数"""
        important_keywords = [
            "important", "key", "main", "primary", "critical",
            "summary", "conclusion", "result", "final",
            "question", "answer", "solution", "problem"
        ]
        
        content_lower = content.lower()
        keyword_count = sum(1 for keyword in important_keywords if keyword in content_lower)
        
        return min(keyword_count / 5.0, 1.0)
    
    def _estimate_tokens(self, text: str) -> int:
        """估算token数量"""
        # 简单的token估算
        if any(ord(char) > 127 for char in text):  # 包含中文
            return len(text)
        else:
            return len(text) // 4
    
    def detect_conversation_drift(self) -> bool:
        """检测对话主题漂移"""
        if len(self.conversation_history) < 6:
            return False
        
        # 比较前半部分和后半部分的相似度
        mid_point = len(self.conversation_history) // 2
        
        early_texts = [
            f"{msg['role']}: {msg['content']}"
            for msg in self.conversation_history[:mid_point]
        ]
        
        late_texts = [
            f"{msg['role']}: {msg['content']}"
            for msg in self.conversation_history[mid_point:]
        ]
        
        early_vector = self.vectorizer.transform([" ".join(early_texts)])
        late_vector = self.vectorizer.transform([" ".join(late_texts)])
        
        similarity = cosine_similarity(early_vector, late_vector)[0][0]
        
        return similarity < 0.5  # 相似度低于0.5认为有漂移
    
    def suggest_context_reset(self) -> bool:
        """建议重置上下文"""
        # 检查多个条件
        conditions = [
            len(self.conversation_history) > 20,  # 对话过长
            self.detect_conversation_drift(),  # 主题漂移
            self._calculate_total_tokens() > 8000  # token过多
        ]
        
        return any(conditions)
    
    def _calculate_total_tokens(self) -> int:
        """计算总token数"""
        total_tokens = 0
        for message in self.conversation_history:
            total_tokens += self._estimate_tokens(message["content"])
        return total_tokens
    
    def get_conversation_summary(self) -> str:
        """获取对话摘要"""
        if not self.conversation_history:
            return "No conversation history"
        
        # 提取关键信息
        user_messages = [msg for msg in self.conversation_history if msg["role"] == "user"]
        assistant_messages = [msg for msg in self.conversation_history if msg["role"] == "assistant"]
        
        # 生成摘要
        summary_parts = []
        
        if user_messages:
            summary_parts.append(f"User asked {len(user_messages)} questions")
        
        if assistant_messages:
            summary_parts.append(f"Assistant provided {len(assistant_messages)} responses")
        
        # 提取主要话题
        all_content = " ".join([msg["content"] for msg in self.conversation_history])
        main_topics = self._extract_main_topics(all_content)
        
        if main_topics:
            summary_parts.append(f"Main topics: {', '.join(main_topics)}")
        
        # 计算对话时长
        if len(self.conversation_history) >= 2:
            start_time = self.conversation_history[0]["timestamp"]
            end_time = self.conversation_history[-1]["timestamp"]
            duration = end_time - start_time
            summary_parts.append(f"Duration: {duration:.1f} seconds")
        
        return ". ".join(summary_parts)
    
    def _extract_main_topics(self, content: str) -> List[str]:
        """提取主要话题"""
        # 简单的关键词提取
        common_topics = [
            "AI", "model", "training", "data", "algorithm",
            "system", "design", "development", "testing",
            "performance", "optimization", "analysis"
        ]
        
        found_topics = []
        content_lower = content.lower()
        
        for topic in common_topics:
            if topic.lower() in content_lower:
                found_topics.append(topic)
        
        return found_topics[:5]  # 最多5个话题
```

## 4. 监控和诊断优化

### 4.1 性能监控

**问题分析：**
- 缺乏实时性能监控
- 问题定位困难
- 性能瓶颈识别不及时
- 趋势分析不足

**优化方案：**

1. **综合性能监控系统**
```python
import time
import threading
import psutil
import asyncio
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from collections import deque
import json
import logging

@dataclass
class PerformanceMetrics:
    timestamp: float
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    disk_usage_percent: float
    network_bytes_sent: int
    network_bytes_recv: int
    active_connections: int
    request_count: int
    response_time_avg: float
    error_rate: float
    cache_hit_rate: float
    llm_call_count: int
    llm_avg_response_time: float
    tool_execution_count: int
    tool_avg_execution_time: float

class PerformanceMonitor:
    def __init__(self, 
                 collection_interval: float = 5.0,
                 history_size: int = 1000,
                 alert_thresholds: Dict[str, float] = None):
        self.collection_interval = collection_interval
        self.history_size = history_size
        self.alert_thresholds = alert_thresholds or {
            "cpu_percent": 80.0,
            "memory_percent": 85.0,
            "disk_usage_percent": 90.0,
            "error_rate": 0.1,
            "response_time_avg": 5.0
        }
        
        self.metrics_history = deque(maxlen=history_size)
        self.current_metrics = None
        self.is_running = False
        self.collection_thread = None
        self.alert_handlers = []
        
        # 业务指标
        self.business_metrics = {
            "request_count": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_response_time": 0.0,
            "llm_call_count": 0,
            "llm_total_time": 0.0,
            "tool_execution_count": 0,
            "tool_total_time": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        
        # 设置日志
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def start(self):
        """启动监控"""
        if self.is_running:
            return
        
        self.is_running = True
        self.collection_thread = threading.Thread(target=self._collection_loop)
        self.collection_thread.daemon = True
        self.collection_thread.start()
        
        self.logger.info("Performance monitor started")
    
    def stop(self):
        """停止监控"""
        self.is_running = False
        if self.collection_thread:
            self.collection_thread.join(timeout=5.0)
        
        self.logger.info("Performance monitor stopped")
    
    def _collection_loop(self):
        """数据收集循环"""
        while self.is_running:
            try:
                # 收集系统指标
                system_metrics = self._collect_system_metrics()
                
                # 收集业务指标
                business_metrics = self._calculate_business_metrics()
                
                # 组合指标
                current_metrics = PerformanceMetrics(
                    timestamp=time.time(),
                    **system_metrics,
                    **business_metrics
                )
                
                self.current_metrics = current_metrics
                self.metrics_history.append(current_metrics)
                
                # 检查告警条件
                self._check_alerts(current_metrics)
                
                time.sleep(self.collection_interval)
                
            except Exception as e:
                self.logger.error(f"Error collecting metrics: {e}")
                time.sleep(self.collection_interval)
    
    def _collect_system_metrics(self) -> Dict[str, Any]:
        """收集系统指标"""
        try:
            # CPU使用率
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # 内存使用情况
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_used_mb = memory.used / (1024 * 1024)
            
            # 磁盘使用情况
            disk = psutil.disk_usage('/')
            disk_usage_percent = disk.percent
            
            # 网络使用情况
            network = psutil.net_io_counters()
            
            # 活跃连接数
            active_connections = len(psutil.net_connections())
            
            return {
                "cpu_percent": cpu_percent,
                "memory_percent": memory_percent,
                "memory_used_mb": memory_used_mb,
                "disk_usage_percent": disk_usage_percent,
                "network_bytes_sent": network.bytes_sent,
                "network_bytes_recv": network.bytes_recv,
                "active_connections": active_connections
            }
            
        except Exception as e:
            self.logger.error(f"Error collecting system metrics: {e}")
            return {
                "cpu_percent": 0.0,
                "memory_percent": 0.0,
                "memory_used_mb": 0.0,
                "disk_usage_percent": 0.0,
                "network_bytes_sent": 0,
                "network_bytes_recv": 0,
                "active_connections": 0
            }
    
    def _calculate_business_metrics(self) -> Dict[str, Any]:
        """计算业务指标"""
        business_metrics = self.business_metrics.copy()
        
        # 计算平均值
        total_requests = business_metrics["request_count"]
        
        if total_requests > 0:
            business_metrics["response_time_avg"] = (
                business_metrics["total_response_time"] / total_requests
            )
            business_metrics["error_rate"] = (
                business_metrics["failed_requests"] / total_requests
            )
        else:
            business_metrics["response_time_avg"] = 0.0
            business_metrics["error_rate"] = 0.0
        
        # 计算缓存命中率
        total_cache_requests = (
            business_metrics["cache_hits"] + business_metrics["cache_misses"]
        )
        
        if total_cache_requests > 0:
            business_metrics["cache_hit_rate"] = (
                business_metrics["cache_hits"] / total_cache_requests
            )
        else:
            business_metrics["cache_hit_rate"] = 0.0
        
        # 计算LLM平均响应时间
        if business_metrics["llm_call_count"] > 0:
            business_metrics["llm_avg_response_time"] = (
                business_metrics["llm_total_time"] / business_metrics["llm_call_count"]
            )
        else:
            business_metrics["llm_avg_response_time"] = 0.0
        
        # 计算工具平均执行时间
        if business_metrics["tool_execution_count"] > 0:
            business_metrics["tool_avg_execution_time"] = (
                business_metrics["tool_total_time"] / business_metrics["tool_execution_count"]
            )
        else:
            business_metrics["tool_avg_execution_time"] = 0.0
        
        return business_metrics
    
    def record_request(self, response_time: float, success: bool = True):
        """记录请求"""
        self.business_metrics["request_count"] += 1
        self.business_metrics["total_response_time"] += response_time
        
        if success:
            self.business_metrics["successful_requests"] += 1
        else:
            self.business_metrics["failed_requests"] += 1
    
    def record_llm_call(self, response_time: float):
        """记录LLM调用"""
        self.business_metrics["llm_call_count"] += 1
        self.business_metrics["llm_total_time"] += response_time
    
    def record_tool_execution(self, execution_time: float):
        """记录工具执行"""
        self.business_metrics["tool_execution_count"] += 1
        self.business_metrics["tool_total_time"] += execution_time
    
    def record_cache_hit(self):
        """记录缓存命中"""
        self.business_metrics["cache_hits"] += 1
    
    def record_cache_miss(self):
        """记录缓存未命中"""
        self.business_metrics["cache_misses"] += 1
    
    def _check_alerts(self, metrics: PerformanceMetrics):
        """检查告警条件"""
        alerts = []
        
        # CPU告警
        if metrics.cpu_percent > self.alert_thresholds["cpu_percent"]:
            alerts.append({
                "type": "cpu_high",
                "message": f"CPU usage is {metrics.cpu_percent:.1f}%",
                "severity": "warning"
            })
        
        # 内存告警
        if metrics.memory_percent > self.alert_thresholds["memory_percent"]:
            alerts.append({
                "type": "memory_high",
                "message": f"Memory usage is {metrics.memory_percent:.1f}%",
                "severity": "warning"
            })
        
        # 磁盘告警
        if metrics.disk_usage_percent > self.alert_thresholds["disk_usage_percent"]:
            alerts.append({
                "type": "disk_high",
                "message": f"Disk usage is {metrics.disk_usage_percent:.1f}%",
                "severity": "critical"
            })
        
        # 错误率告警
        if metrics.error_rate > self.alert_thresholds["error_rate"]:
            alerts.append({
                "type": "error_rate_high",
                "message": f"Error rate is {metrics.error_rate:.2%}",
                "severity": "warning"
            })
        
        # 响应时间告警
        if metrics.response_time_avg > self.alert_thresholds["response_time_avg"]:
            alerts.append({
                "type": "response_time_high",
                "message": f"Average response time is {metrics.response_time_avg:.2f}s",
                "severity": "warning"
            })
        
        # 发送告警
        for alert in alerts:
            self._send_alert(alert)
    
    def _send_alert(self, alert: Dict[str, Any]):
        """发送告警"""
        self.logger.warning(f"ALERT: {alert['message']}")
        
        # 调用告警处理器
        for handler in self.alert_handlers:
            try:
                handler(alert)
            except Exception as e:
                self.logger.error(f"Error in alert handler: {e}")
    
    def add_alert_handler(self, handler):
        """添加告警处理器"""
        self.alert_handlers.append(handler)
    
    def get_current_metrics(self) -> Optional[PerformanceMetrics]:
        """获取当前指标"""
        return self.current_metrics
    
    def get_metrics_history(self, 
                           start_time: Optional[float] = None,
                           end_time: Optional[float] = None) -> List[PerformanceMetrics]:
        """获取历史指标"""
        if not start_time and not end_time:
            return list(self.metrics_history)
        
        filtered_metrics = []
        for metrics in self.metrics_history:
            if start_time and metrics.timestamp < start_time:
                continue
            if end_time and metrics.timestamp > end_time:
                continue
            filtered_metrics.append(metrics)
        
        return filtered_metrics
    
    def get_performance_summary(self, duration: float = 3600) -> Dict[str, Any]:
        """获取性能摘要"""
        end_time = time.time()
        start_time = end_time - duration
        
        metrics_in_period = self.get_metrics_history(start_time, end_time)
        
        if not metrics_in_period:
            return {"error": "No metrics data available for the specified period"}
        
        # 计算摘要统计
        cpu_values = [m.cpu_percent for m in metrics_in_period]
        memory_values = [m.memory_percent for m in metrics_in_period]
        response_times = [m.response_time_avg for m in metrics_in_period if m.response_time_avg > 0]
        
        summary = {
            "period_start": start_time,
            "period_end": end_time,
            "duration": duration,
            "total_requests": sum(m.request_count for m in metrics_in_period),
            "avg_cpu_percent": np.mean(cpu_values) if cpu_values else 0,
            "max_cpu_percent": max(cpu_values) if cpu_values else 0,
            "avg_memory_percent": np.mean(memory_values) if memory_values else 0,
            "max_memory_percent": max(memory_values) if memory_values else 0,
            "avg_response_time": np.mean(response_times) if response_times else 0,
            "max_response_time": max(response_times) if response_times else 0,
            "total_errors": sum(m.failed_requests for m in metrics_in_period),
            "error_rate": sum(m.failed_requests for m in metrics_in_period) / 
                         max(sum(m.request_count for m in metrics_in_period), 1),
            "cache_hit_rate": np.mean([m.cache_hit_rate for m in metrics_in_period]) if metrics_in_period else 0
        }
        
        return summary
    
    def export_metrics(self, file_path: str, format: str = "json"):
        """导出指标数据"""
        metrics_data = [asdict(m) for m in self.metrics_history]
        
        if format.lower() == "json":
            with open(file_path, 'w') as f:
                json.dump(metrics_data, f, indent=2)
        elif format.lower() == "csv":
            import pandas as pd
            df = pd.DataFrame(metrics_data)
            df.to_csv(file_path, index=False)
        else:
            raise ValueError(f"Unsupported format: {format}")
        
        self.logger.info(f"Metrics exported to {file_path}")
```

2. **实时告警系统**
```python
import smtplib
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
import requests
import logging

class AlertManager:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.alert_channels = {}
        self.alert_history = []
        self.alert_suppressions = {}
    
    def add_email_channel(self, 
                          name: str,
                          smtp_server: str,
                          smtp_port: int,
                          username: str,
                          password: str,
                          recipients: List[str]):
        """添加邮件告警通道"""
        self.alert_channels[name] = {
            "type": "email",
            "smtp_server": smtp_server,
            "smtp_port": smtp_port,
            "username": username,
            "password": password,
            "recipients": recipients
        }
    
    def add_webhook_channel(self, name: str, webhook_url: str):
        """添加Webhook告警通道"""
        self.alert_channels[name] = {
            "type": "webhook",
            "url": webhook_url
        }
    
    def add_slack_channel(self, name: str, webhook_url: str):
        """添加Slack告警通道"""
        self.alert_channels[name] = {
            "type": "slack",
            "url": webhook_url
        }
    
    def send_alert(self, alert: Dict[str, Any], channels: List[str] = None):
        """发送告警"""
        if channels is None:
            channels = list(self.alert_channels.keys())
        
        # 检查告警抑制
        if self._is_alert_suppressed(alert):
            return
        
        # 记录告警历史
        self.alert_history.append({
            "timestamp": time.time(),
            "alert": alert,
            "channels": channels
        })
        
        # 发送到各个通道
        for channel_name in channels:
            if channel_name in self.alert_channels:
                try:
                    self._send_to_channel(channel_name, alert)
                except Exception as e:
                    self.logger.error(f"Failed to send alert to {channel_name}: {e}")
    
    def _send_to_channel(self, channel_name: str, alert: Dict[str, Any]):
        """发送告警到指定通道"""
        channel = self.alert_channels[channel_name]
        
        if channel["type"] == "email":
            self._send_email_alert(channel, alert)
        elif channel["type"] == "webhook":
            self._send_webhook_alert(channel, alert)
        elif channel["type"] == "slack":
            self._send_slack_alert(channel, alert)
    
    def _send_email_alert(self, channel: Dict, alert: Dict[str, Any]):
        """发送邮件告警"""
        msg = MIMEMultipart()
        msg['From'] = channel['username']
        msg['To'] = ', '.join(channel['recipients'])
        msg['Subject'] = f"Qwen-Agent Alert: {alert['type']}"
        
        # 构建邮件内容
        body = f"""
        Alert Type: {alert['type']}
        Severity: {alert['severity']}
        Message: {alert['message']}
        Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
        
        This alert was generated by the Qwen-Agent monitoring system.
        """
        
        msg.attach(MimeText(body, 'plain'))
        
        # 发送邮件
        with smtplib.SMTP(channel['smtp_server'], channel['smtp_port']) as server:
            server.starttls()
            server.login(channel['username'], channel['password'])
            server.send_message(msg)
    
    def _send_webhook_alert(self, channel: Dict, alert: Dict[str, Any]):
        """发送Webhook告警"""
        payload = {
            "alert_type": alert['type'],
            "severity": alert['severity'],
            "message": alert['message'],
            "timestamp": time.time(),
            "source": "qwen-agent"
        }
        
        response = requests.post(
            channel['url'],
            json=payload,
            timeout=10
        )
        response.raise_for_status()
    
    def _send_slack_alert(self, channel: Dict, alert: Dict[str, Any]):
        """发送Slack告警"""
        # 根据严重程度选择颜色
        color_map = {
            "info": "#36a64f",
            "warning": "#ff9500",
            "critical": "#ff0000"
        }
        
        color = color_map.get(alert['severity'], "#36a64f")
        
        payload = {
            "attachments": [
                {
                    "color": color,
                    "title": f"Qwen-Agent Alert: {alert['type']}",
                    "text": alert['message'],
                    "ts": int(time.time())
                }
            ]
        }
        
        response = requests.post(
            channel['url'],
            json=payload,
            timeout=10
        )
        response.raise_for_status()
    
    def _is_alert_suppressed(self, alert: Dict[str, Any]) -> bool:
        """检查告警是否被抑制"""
        alert_key = f"{alert['type']}_{alert.get('severity', 'info')}"
        
        if alert_key not in self.alert_suppressions:
            return False
        
        suppression = self.alert_suppressions[alert_key]
        
        # 检查抑制时间
        if time.time() > suppression['until']:
            del self.alert_suppressions[alert_key]
            return False
        
        return True
    
    def suppress_alert(self, alert_type: str, severity: str, duration: float = 300):
        """抑制告警"""
        alert_key = f"{alert_type}_{severity}"
        
        self.alert_suppressions[alert_key] = {
            "until": time.time() + duration
        }
        
        self.logger.info(f"Alert {alert_key} suppressed for {duration} seconds")
    
    def get_alert_history(self, 
                          start_time: Optional[float] = None,
                          end_time: Optional[float] = None,
                          alert_type: Optional[str] = None) -> List[Dict]:
        """获取告警历史"""
        filtered_history = []
        
        for alert_record in self.alert_history:
            # 时间过滤
            if start_time and alert_record['timestamp'] < start_time:
                continue
            if end_time and alert_record['timestamp'] > end_time:
                continue
            
            # 类型过滤
            if alert_type and alert_record['alert']['type'] != alert_type:
                continue
            
            filtered_history.append(alert_record)
        
        return filtered_history
```

## 5. 安全性优化

### 5.1 输入验证和清理

**问题分析：**
- 输入注入攻击风险
- 敏感信息泄露
- 恶意代码执行
- 数据完整性问题

**优化方案：**

1. **输入验证系统**
```python
import re
import html
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import json

@dataclass
class ValidationResult:
    is_valid: bool
    errors: List[str]
    sanitized_data: Any
    risk_level: str  # low, medium, high

class InputValidator:
    def __init__(self):
        self.validation_rules = {
            "text": self._validate_text,
            "code": self._validate_code,
            "file_path": self._validate_file_path,
            "url": self._validate_url,
            "json": self._validate_json
        }
        
        self.sensitive_patterns = [
            r"password\s*[:=]\s*[\"'].*?[\"']",  # 密码
            r"api_key\s*[:=]\s*[\"'].*?[\"']",  # API密钥
            r"secret\s*[:=]\s*[\"'].*?[\"']",  # 密钥
            r"token\s*[:=]\s*[\"'].*?[\"']",   # 令牌
            r"private_key\s*[:=]\s*[\"'].*?[\"']",  # 私钥
        ]
        
        self.malicious_patterns = [
            r"<script[^>]*>.*?</script>",  # XSS脚本
            r"javascript:",  # JavaScript协议
            r"vbscript:",    # VBScript协议
            r"onload\s*=",  # 事件处理器
            r"onerror\s*=",  # 错误处理器
            r"eval\s*\(",   # eval函数
            r"exec\s*\(",    # exec函数
            r"system\s*\(",  # system函数
            r"subprocess\s*\(",  # subprocess函数
            r"os\.system\s*\(",  # os.system
        ]
    
    def validate_input(self, 
                      data: Any, 
                      data_type: str = "text",
                      context: Dict[str, Any] = None) -> ValidationResult:
        """验证输入数据"""
        context = context or {}
        
        # 检查数据类型
        if data_type not in self.validation_rules:
            return ValidationResult(
                is_valid=False,
                errors=[f"Unsupported data type: {data_type}"],
                sanitized_data=data,
                risk_level="high"
            )
        
        # 执行类型特定的验证
        validator = self.validation_rules[data_type]
        return validator(data, context)
    
    def _validate_text(self, text: str, context: Dict[str, Any]) -> ValidationResult:
        """验证文本输入"""
        errors = []
        risk_level = "low"
        sanitized_text = text
        
        # 基本检查
        if not isinstance(text, str):
            errors.append("Input must be a string")
            risk_level = "high"
            return ValidationResult(False, errors, text, risk_level)
        
        # 长度检查
        max_length = context.get("max_length", 10000)
        if len(text) > max_length:
            errors.append(f"Text too long (max {max_length} characters)")
            risk_level = "medium"
        
        # 检查敏感信息
        sensitive_found = self._check_sensitive_patterns(text)
        if sensitive_found:
            errors.append(f"Contains sensitive information: {', '.join(sensitive_found)}")
            risk_level = "high"
            # 屏蔽敏感信息
            sanitized_text = self._sanitize_sensitive_info(text)
        
        # 检查恶意内容
        malicious_found = self._check_malicious_patterns(text)
        if malicious_found:
            errors.append(f"Contains potentially malicious content: {', '.join(malicious_found)}")
            risk_level = "high"
            # 清理恶意内容
            sanitized_text = self._sanitize_malicious_content(sanitized_text)
        
        # HTML转义（如果需要）
        if context.get("escape_html", False):
            sanitized_text = html.escape(sanitized_text)
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            sanitized_data=sanitized_text,
            risk_level=risk_level
        )
    
    def _validate_code(self, code: str, context: Dict[str, Any]) -> ValidationResult:
        """验证代码输入"""
        errors = []
        risk_level = "medium"
        sanitized_code = code
        
        # 基本检查
        if not isinstance(code, str):
            errors.append("Code must be a string")
            risk_level = "high"
            return ValidationResult(False, errors, code, risk_level)
        
        # 检查代码长度
        max_length = context.get("max_code_length", 5000)
        if len(code) > max_length:
            errors.append(f"Code too long (max {max_length} characters)")
            risk_level = "high"
        
        # 检查危险函数
        dangerous_functions = [
            "eval", "exec", "compile", "execfile",
            "input", "raw_input", "__import__",
            "open", "file", "os.system", "subprocess",
            "commands.getoutput", "os.popen"
        ]
        
        found_dangerous = []
        for func in dangerous_functions:
            if re.search(rf"\b{func}\b", code):
                found_dangerous.append(func)
        
        if found_dangerous:
            errors.append(f"Contains dangerous functions: {', '.join(found_dangerous)}")
            risk_level = "high"
        
        # 检查文件操作
        file_operations = re.findall(r"(open|file)\s*\([^)]*\)", code)
        if file_operations:
            errors.append("Contains file operations")
            risk_level = "high"
        
        # 检查网络操作
        network_operations = re.findall(r"(urllib|requests|socket)\.", code)
        if network_operations:
            errors.append("Contains network operations")
            risk_level = "medium"
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            sanitized_data=sanitized_code,
            risk_level=risk_level
        )
    
    def _validate_file_path(self, file_path: str, context: Dict[str, Any]) -> ValidationResult:
        """验证文件路径"""
        errors = []
        risk_level = "low"
        sanitized_path = file_path
        
        # 基本检查
        if not isinstance(file_path, str):
            errors.append("File path must be a string")
            risk_level = "high"
            return ValidationResult(False, errors, file_path, risk_level)
        
        # 检查路径遍历攻击
        if ".." in file_path:
            errors.append("Path traversal attempt detected")
            risk_level = "high"
            # 清理路径
            sanitized_path = os.path.normpath(file_path)
        
        # 检查绝对路径（如果需要）
        if context.get("allow_absolute", False) == False:
            if os.path.isabs(file_path):
                errors.append("Absolute paths not allowed")
                risk_level = "medium"
        
        # 检查文件扩展名
        allowed_extensions = context.get("allowed_extensions", [])
        if allowed_extensions:
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext not in allowed_extensions:
                errors.append(f"File extension {file_ext} not allowed")
                risk_level = "medium"
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            sanitized_data=sanitized_path,
            risk_level=risk_level
        )
    
    def _validate_url(self, url: str, context: Dict[str, Any]) -> ValidationResult:
        """验证URL"""
        errors = []
        risk_level = "low"
        sanitized_url = url
        
        # 基本检查
        if not isinstance(url, str):
            errors.append("URL must be a string")
            risk_level = "high"
            return ValidationResult(False, errors, url, risk_level)
        
        # 检查URL格式
        url_pattern = re.compile(
            r'^https?://'  # 协议
            r'(?:[-\w.]|(?:%[\da-fA-F]{2}))+'  # 域名
            r'(?::\d{2,5})?'  # 端口
            r'(?:[/?#][^\s]*)?'  # 路径
            r'$'
        )
        
        if not url_pattern.match(url):
            errors.append("Invalid URL format")
            risk_level = "high"
        
        # 检查允许的域名
        allowed_domains = context.get("allowed_domains", [])
        if allowed_domains:
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            if parsed_url.netloc not in allowed_domains:
                errors.append(f"Domain {parsed_url.netloc} not allowed")
                risk_level = "medium"
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            sanitized_data=sanitized_url,
            risk_level=risk_level
        )
    
    def _validate_json(self, json_str: str, context: Dict[str, Any]) -> ValidationResult:
        """验证JSON"""
        errors = []
        risk_level = "low"
        sanitized_json = json_str
        
        # 基本检查
        if not isinstance(json_str, str):
            errors.append("JSON must be a string")
            risk_level = "high"
            return ValidationResult(False, errors, json_str, risk_level)
        
        # 尝试解析JSON
        try:
            parsed_json = json.loads(json_str)
            
            # 检查JSON结构
            expected_schema = context.get("expected_schema")
            if expected_schema:
                schema_errors = self._validate_json_schema(parsed_json, expected_schema)
                errors.extend(schema_errors)
                if schema_errors:
                    risk_level = "medium"
            
            # 检查敏感字段
            sensitive_fields = context.get("sensitive_fields", [])
            if sensitive_fields:
                sensitive_found = self._check_json_sensitive_fields(parsed_json, sensitive_fields)
                if sensitive_found:
                    errors.append(f"Contains sensitive fields: {', '.join(sensitive_found)}")
                    risk_level = "high"
                    sanitized_json = self._sanitize_json_sensitive_fields(parsed_json, sensitive_fields)
            
        except json.JSONDecodeError as e:
            errors.append(f"Invalid JSON: {str(e)}")
            risk_level = "high"
        
        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            sanitized_data=sanitized_json,
            risk_level=risk_level
        )
    
    def _check_sensitive_patterns(self, text: str) -> List[str]:
        """检查敏感信息模式"""
        found = []
        
        for pattern in self.sensitive_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                found.extend(matches)
        
        return found
    
    def _check_malicious_patterns(self, text: str) -> List[str]:
        """检查恶意模式"""
        found = []
        
        for pattern in self.malicious_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                found.extend(matches)
        
        return found
    
    def _sanitize_sensitive_info(self, text: str) -> str:
        """清理敏感信息"""
        sanitized = text
        
        # 替换敏感信息
        for pattern in self.sensitive_patterns:
            sanitized = re.sub(pattern, "***REDACTED***", sanitized)
        
        return sanitized
    
    def _sanitize_malicious_content(self, text: str) -> str:
        """清理恶意内容"""
        sanitized = text
        
        # 移除恶意脚本
        sanitized = re.sub(r'<script[^>]*>.*?</script>', '', sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # 移除JavaScript协议
        sanitized = re.sub(r'javascript:', '[REMOVED]', sanitized, flags=re.IGNORECASE)
        
        # 移除事件处理器
        sanitized = re.sub(r'on\w+\s*=', '[REMOVED]', sanitized, flags=re.IGNORECASE)
        
        return sanitized
    
    def _validate_json_schema(self, json_data: Any, schema: Dict) -> List[str]:
        """验证JSON模式"""
        errors = []
        
        def validate_recursive(data, schema, path=""):
            if isinstance(schema, dict):
                if "type" in schema:
                    expected_type = schema["type"]
                    if expected_type == "object":
                        if not isinstance(data, dict):
                            errors.append(f"{path}: Expected object, got {type(data)}")
                        else:
                            for key, value_schema in schema.get("properties", {}).items():
                                if key in data:
                                    validate_recursive(data[key], value_schema, f"{path}.{key}")
                    elif expected_type == "array":
                        if not isinstance(data, list):
                            errors.append(f"{path}: Expected array, got {type(data)}")
                        else:
                            if "items" in schema:
                                for i, item in enumerate(data):
                                    validate_recursive(item, schema["items"], f"{path}[{i}]")
                    elif expected_type == "string":
                        if not isinstance(data, str):
                            errors.append(f"{path}: Expected string, got {type(data)}")
                    elif expected_type == "number":
                        if not isinstance(data, (int, float)):
                            errors.append(f"{path}: Expected number, got {type(data)}")
                    elif expected_type == "boolean":
                        if not isinstance(data, bool):
                            errors.append(f"{path}: Expected boolean, got {type(data)}")
        
        validate_recursive(json_data, schema)
        return errors
    
    def _check_json_sensitive_fields(self, json_data: Any, sensitive_fields: List[str]) -> List[str]:
        """检查JSON中的敏感字段"""
        found = []
        
        def check_recursive(data, path=""):
            if isinstance(data, dict):
                for key, value in data.items():
                    if key in sensitive_fields:
                        found.append(f"{path}.{key}")
                    check_recursive(value, f"{path}.{key}")
            elif isinstance(data, list):
                for i, item in enumerate(data):
                    check_recursive(item, f"{path}[{i}]")
        
        check_recursive(json_data)
        return found
    
    def _sanitize_json_sensitive_fields(self, json_data: Any, sensitive_fields: List[str]) -> str:
        """清理JSON中的敏感字段"""
        def sanitize_recursive(data):
            if isinstance(data, dict):
                sanitized = {}
                for key, value in data.items():
                    if key in sensitive_fields:
                        sanitized[key] = "***REDACTED***"
                    else:
                        sanitized[key] = sanitize_recursive(value)
                return sanitized
            elif isinstance(data, list):
                return [sanitize_recursive(item) for item in data]
            else:
                return data
        
        sanitized_data = sanitize_recursive(json_data)
        return json.dumps(sanitized_data)
```

### 5.2 权限控制

**问题分析：**
- 缺乏细粒度权限控制
- 资源访问控制不足
- 操作审计不完善
- 角色管理不够灵活

**优化方案：**

1. **基于角色的访问控制（RBAC）**
```python
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import time
import json

class Permission(Enum):
    # Agent权限
    CREATE_AGENT = "create_agent"
    READ_AGENT = "read_agent"
    UPDATE_AGENT = "update_agent"
    DELETE_AGENT = "delete_agent"
    EXECUTE_AGENT = "execute_agent"
    
    # LLM权限
    USE_LLM = "use_llm"
    CONFIGURE_LLM = "configure_llm"
    MANAGE_LLM_KEYS = "manage_llm_keys"
    
    # 工具权限
    USE_TOOLS = "use_tools"
    REGISTER_TOOL = "register_tool"
    EXECUTE_TOOL = "execute_tool"
    
    # 系统权限
    READ_SYSTEM_METRICS = "read_system_metrics"
    MANAGE_SYSTEM_CONFIG = "manage_system_config"
    MANAGE_USERS = "manage_users"
    
    # 数据权限
    READ_DATA = "read_data"
    WRITE_DATA = "write_data"
    DELETE_DATA = "delete_data"
    EXPORT_DATA = "export_data"

@dataclass
class Role:
    name: str
    description: str
    permissions: Set[Permission] = field(default_factory=set)
    inherits_from: List[str] = field(default_factory=list)

@dataclass
class User:
    user_id: str
    username: str
    email: str
    roles: List[str]
    is_active: bool = True
    created_at: float = field(default_factory=time.time)
    last_login: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

class RBACManager:
    def __init__(self):
        self.roles: Dict[str, Role] = {}
        self.users: Dict[str, User] = {}
        self.role_permissions_cache: Dict[str, Set[Permission]] = {}
        
        # 初始化默认角色
        self._initialize_default_roles()
    
    def _initialize_default_roles(self):
        """初始化默认角色"""
        # 超级管理员
        admin_role = Role(
            name="admin",
            description="Super administrator with full access",
            permissions=set(Permission),  # 所有权限
            inherits_from=[]
        )
        
        # 系统管理员
        sysadmin_role = Role(
            name="sysadmin",
            description="System administrator",
            permissions={
                Permission.READ_SYSTEM_METRICS,
                Permission.MANAGE_SYSTEM_CONFIG,
                Permission.MANAGE_USERS,
                Permission.CREATE_AGENT,
                Permission.READ_AGENT,
                Permission.UPDATE_AGENT,
                Permission.DELETE_AGENT,
                Permission.USE_LLM,
                Permission.CONFIGURE_LLM,
                Permission.USE_TOOLS,
                Permission.REGISTER_TOOL,
                Permission.READ_DATA,
                Permission.WRITE_DATA,
                Permission.DELETE_DATA,
                Permission.EXPORT_DATA
            },
            inherits_from=[]
        )
        
        # 开发者
        developer_role = Role(
            name="developer",
            description="Developer role",
            permissions={
                Permission.CREATE_AGENT,
                Permission.READ_AGENT,
                Permission.UPDATE_AGENT,
                Permission.EXECUTE_AGENT,
                Permission.USE_LLM,
                Permission.USE_TOOLS,
                Permission.REGISTER_TOOL,
                Permission.READ_DATA,
                Permission.WRITE_DATA,
                Permission.EXPORT_DATA
            },
            inherits_from=[]
        )
        
        # 普通用户
        user_role = Role(
            name="user",
            description="Regular user",
            permissions={
                Permission.READ_AGENT,
                Permission.EXECUTE_AGENT,
                Permission.USE_LLM,
                Permission.USE_TOOLS,
                Permission.READ_DATA
            },
            inherits_from=[]
        )
        
        # 只读用户
        readonly_role = Role(
            name="readonly",
            description="Read-only user",
            permissions={
                Permission.READ_AGENT,
                Permission.READ_DATA
            },
            inherits_from=[]
        )
        
        # 注册角色
        for role in [admin_role, sysadmin_role, developer_role, user_role, readonly_role]:
            self.add_role(role)
    
    def add_role(self, role: Role):
        """添加角色"""
        self.roles[role.name] = role
        # 清除缓存
        self.role_permissions_cache.clear()
    
    def add_user(self, user: User):
        """添加用户"""
        self.users[user.user_id] = user
    
    def get_user(self, user_id: str) -> Optional[User]:
        """获取用户"""
        return self.users.get(user_id)
    
    def get_user_permissions(self, user_id: str) -> Set[Permission]:
        """获取用户权限"""
        user = self.get_user(user_id)
        if not user:
            return set()
        
        # 检查缓存
        cache_key = f"{user_id}_permissions"
        if cache_key in self.role_permissions_cache:
            return self.role_permissions_cache[cache_key]
        
        # 计算权限
        permissions = set()
        
        for role_name in user.roles:
            role_permissions = self._get_role_permissions(role_name)
            permissions.update(role_permissions)
        
        # 缓存结果
        self.role_permissions_cache[cache_key] = permissions
        
        return permissions
    
    def _get_role_permissions(self, role_name: str) -> Set[Permission]:
        """获取角色权限（包括继承的权限）"""
        if role_name not in self.roles:
            return set()
        
        role = self.roles[role_name]
        permissions = set(role.permissions)
        
        # 处理继承
        for parent_role_name in role.inherits_from:
            parent_permissions = self._get_role_permissions(parent_role_name)
            permissions.update(parent_permissions)
        
        return permissions
    
    def check_permission(self, user_id: str, permission: Permission) -> bool:
        """检查用户是否有特定权限"""
        user_permissions = self.get_user_permissions(user_id)
        return permission in user_permissions
    
    def check_permissions(self, user_id: str, permissions: List[Permission]) -> bool:
        """检查用户是否有所有指定权限"""
        user_permissions = self.get_user_permissions(user_id)
        return all(perm in user_permissions for perm in permissions)
    
    def add_user_role(self, user_id: str, role_name: str):
        """为用户添加角色"""
        user = self.get_user(user_id)
        if user and role_name in self.roles:
            if role_name not in user.roles:
                user.roles.append(role_name)
                # 清除缓存
                self.role_permissions_cache.clear()
    
    def remove_user_role(self, user_id: str, role_name: str):
        """移除用户角色"""
        user = self.get_user(user_id)
        if user and role_name in user.roles:
            user.roles.remove(role_name)
            # 清除缓存
            self.role_permissions_cache.clear()
    
    def create_user(self, 
                   username: str, 
                   email: str, 
                   roles: List[str] = None) -> User:
        """创建用户"""
        user_id = self._generate_user_id(username)
        
        user = User(
            user_id=user_id,
            username=username,
            email=email,
            roles=roles or ["user"]
        )
        
        self.add_user(user)
        return user
    
    def _generate_user_id(self, username: str) -> str:
        """生成用户ID"""
        timestamp = str(int(time.time()))
        unique_string = f"{username}_{timestamp}"
        return hashlib.md5(unique_string.encode()).hexdigest()[:16]
    
    def get_users_by_role(self, role_name: str) -> List[User]:
        """获取具有特定角色的用户"""
        return [user for user in self.users.values() if role_name in user.roles]
    
    def update_user_last_login(self, user_id: str):
        """更新用户最后登录时间"""
        user = self.get_user(user_id)
        if user:
            user.last_login = time.time()
    
    def deactivate_user(self, user_id: str):
        """停用用户"""
        user = self.get_user(user_id)
        if user:
            user.is_active = False
    
    def activate_user(self, user_id: str):
        """激活用户"""
        user = self.get_user(user_id)
        if user:
            user.is_active = True
    
    def get_role_users(self, role_name: str) -> List[User]:
        """获取角色下的所有用户"""
        return [user for user in self.users.values() 
                if role_name in user.roles and user.is_active]
    
    def export_config(self, file_path: str):
        """导出配置"""
        config = {
            "roles": {
                name: {
                    "description": role.description,
                    "permissions": [p.value for p in role.permissions],
                    "inherits_from": role.inherits_from
                }
                for name, role in self.roles.items()
            },
            "users": {
                user.user_id: {
                    "username": user.username,
                    "email": user.email,
                    "roles": user.roles,
                    "is_active": user.is_active,
                    "created_at": user.created_at,
                    "last_login": user.last_login,
                    "metadata": user.metadata
                }
                for user in self.users.values()
            }
        }
        
        with open(file_path, 'w') as f:
            json.dump(config, f, indent=2)
    
    def import_config(self, file_path: str):
        """导入配置"""
        with open(file_path, 'r') as f:
            config = json.load(f)
        
        # 导入角色
        for role_name, role_data in config["roles"].items():
            role = Role(
                name=role_name,
                description=role_data["description"],
                permissions=set(Permission(p) for p in role_data["permissions"]),
                inherits_from=role_data.get("inherits_from", [])
            )
            self.add_role(role)
        
        # 导入用户
        for user_id, user_data in config["users"].items():
            user = User(
                user_id=user_id,
                username=user_data["username"],
                email=user_data["email"],
                roles=user_data["roles"],
                is_active=user_data["is_active"],
                created_at=user_data.get("created_at", time.time()),
                last_login=user_data.get("last_login"),
                metadata=user_data.get("metadata", {})
            )
            self.add_user(user)

# 权限装饰器
def require_permission(permission: Permission):
    """权限检查装饰器"""
    def decorator(func):
        def wrapper(self, *args, **kwargs):
            # 获取用户ID（假设在self.user_id中）
            user_id = getattr(self, 'user_id', None)
            
            if not user_id:
                raise PermissionError("User ID not found")
            
            # 检查权限
            rbac_manager = getattr(self, 'rbac_manager', None)
            if not rbac_manager:
                raise PermissionError("RBAC manager not found")
            
            if not rbac_manager.check_permission(user_id, permission):
                raise PermissionError(f"Permission denied: {permission.value}")
            
            return func(self, *args, **kwargs)
        return wrapper
    return decorator

def require_permissions(permissions: List[Permission]):
    """多权限检查装饰器"""
    def decorator(func):
        def wrapper(self, *args, **kwargs):
            user_id = getattr(self, 'user_id', None)
            
            if not user_id:
                raise PermissionError("User ID not found")
            
            rbac_manager = getattr(self, 'rbac_manager', None)
            if not rbac_manager:
                raise PermissionError("RBAC manager not found")
            
            if not rbac_manager.check_permissions(user_id, permissions):
                raise PermissionError(f"Permission denied for required permissions")
            
            return func(self, *args, **kwargs)
        return wrapper
    return decorator

class PermissionError(Exception):
    """权限错误"""
    pass
```

## 6. 总结

通过这些优化建议，Qwen-Agent可以在以下方面得到显著提升：

### 6.1 性能提升
- **响应时间减少**：通过缓存、并发优化和连接池管理
- **资源利用率提高**：通过内存优化和智能资源调度
- **并发能力增强**：通过智能并发控制和负载均衡

### 6.2 架构改进
- **可扩展性提升**：通过微服务化和事件驱动架构
- **可维护性增强**：通过模块化设计和清晰的接口
- **容错性提高**：通过故障隔离和优雅降级

### 6.3 功能增强
- **工具系统优化**：智能工具选择和执行优化
- **对话管理改进**：智能上下文管理和压缩
- **安全性提升**：输入验证和权限控制

### 6.4 运维能力
- **监控完善**：全面的性能监控和告警系统
- **可观测性**：详细的日志和指标收集
- **故障诊断**：快速问题定位和解决

这些优化建议应该根据实际应用场景和需求进行选择和实施。建议分阶段进行优化，先解决最关键的性能问题，然后逐步完善其他方面。